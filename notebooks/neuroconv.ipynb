{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f52409c6",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "If you were running this on your own machine you would have\n",
    "to install jupyter, neuroconv package and the data.\n",
    "\n",
    "However for the workshop these steps have already been done\n",
    "for you so you don't need to install anything.\n",
    "\n",
    "# NWB workshop\n",
    "\n",
    "## Converting to NWB with neuroconv\n",
    "\n",
    "We will deal with three problems that we think arise often.\n",
    "The first one is having a type of data that you would like\n",
    "to convert to NWB format.  The second problem is that you already \n",
    "have an NWB file and some data you would like to add to the NWB file.\n",
    "The third problem is that you have an NWB file and you would like to\n",
    "extract some data from the file.\n",
    "\n",
    "\n",
    "### Problem 1a:\n",
    "We will start with the simplest case scenario.\n",
    "We have a single source of data. The data that we have\n",
    "are from a csv file, \n",
    "to convert them to NWB we will use the `CsvTimeIntervalsInterface`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c1163d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requirement for the conversion\n",
    "from neuroconv.datainterfaces import CsvTimeIntervalsInterface\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a6074",
   "metadata": {},
   "source": [
    "#### Create CSV file if not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e91454c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "data_dir = Path(\"../data\") # Data path\n",
    "data_dir.mkdir(exist_ok=True) # Create data dir\n",
    "csv_file_path = data_dir / \"mydata.csv\" # Form file data path\n",
    "random.seed(42) # set seed\n",
    "if not csv_file_path.exists():\n",
    "    # Create csv file if it doesn't exist\n",
    "    n = 100\n",
    "    starts  = [-1]*n\n",
    "    ends  = [-1]*n\n",
    "    vals = [-1]*n\n",
    "    s0 = 0\n",
    "    for i in range(n):\n",
    "        starts[i]= s0 + random.randrange(5)\n",
    "        ends[i] = starts[i] + random.randrange(1,50)\n",
    "        vals[i] = random.uniform(0,1)\n",
    "        s0 = ends[i]\n",
    "\n",
    "\n",
    "    pd.DataFrame({'start_time':starts, 'end_time':ends, 'value':vals }).to_csv(csv_file_path,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb71c1b",
   "metadata": {},
   "source": [
    "#### Create interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc721eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_interface = CsvTimeIntervalsInterface(file_path=csv_file_path, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dab7a04",
   "metadata": {},
   "source": [
    "#### Get metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb97626",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = csv_interface.get_metadata()\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837e6d6",
   "metadata": {},
   "source": [
    "#### Attempt to create NWB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8730f4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob1a_nwb_file = data_dir/\"problem_1a.nwb\"\n",
    "csv_interface.run_conversion(nwbfile_path = prob1a_nwb_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03926ec9",
   "metadata": {},
   "source": [
    "#### Add missing metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceac4983",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_start_time = datetime(2025,1,10,11,45,0, tzinfo=ZoneInfo(\"Europe/Paris\"))\n",
    "metadata[\"NWBFile\"][\"session_start_time\"] = session_start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e88b33",
   "metadata": {},
   "source": [
    "#### Create NWB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a227e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_interface.run_conversion(nwbfile_path = prob1a_nwb_file, metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d317de",
   "metadata": {},
   "source": [
    "#### Look at the generated NWB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "888c30d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "import pynwb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcf5874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideally we would use a with statement \n",
    "# as in the commented code\n",
    "#\n",
    "## with NWBHDF5IO(prob1a_nwb_file, mode = 'r') as io:\n",
    "##    nwbfile = io.read()\n",
    "##    nwbfile\n",
    "# we instead use the code below to be able to\n",
    "# see a nice version of the file.\n",
    "io = NWBHDF5IO(prob1a_nwb_file, mode = 'r')\n",
    "nwbfile = io.read()\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7407341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close() # Don't forget to close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f655e3b",
   "metadata": {},
   "source": [
    "#### Use nwbwidgets to look at the file\n",
    "Just a taste of how nwbwidgets work, we will take a closer look at nwbwidgets later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ca270f-a280-471d-831a-476805bc876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nwbwidgets import nwb2widget\n",
    "\n",
    "io = NWBHDF5IO(prob1a_nwb_file, mode = 'r')\n",
    "nwbfile = io.read()\n",
    "nwb2widget(nwbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0e1f9-6e7d-44c2-8110-4dfe883120e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ec48c3",
   "metadata": {},
   "source": [
    "###  Problem 1b\n",
    "Now we again want to generate a NWBFile from already available data.\n",
    "However, in addition to the csv file with some tiff images that \n",
    "should also be inside the generated NWB file.\n",
    "#### Download tiff file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017979b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download movie file if not already available\n",
    "if [[ ! -e \"../data/demoMovie.tif\" ]]; then\n",
    "   wget https://github.com/flatironinstitute/CaImAn/raw/refs/heads/main/example_movies/demoMovie.tif -O ../data/demoMovie.tif\n",
    "fi        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1d7b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroconv.datainterfaces import TiffImagingInterface\n",
    "from neuroconv import NWBConverter\n",
    "movie_path = data_dir / 'demoMovie.tif'\n",
    "prob1b_nwb_file = data_dir / 'problem_1b.nwb'\n",
    "\n",
    "\n",
    "class MyConverter(NWBConverter):\n",
    "    data_interface_classes = dict (\n",
    "        csvIntervals = CsvTimeIntervalsInterface,\n",
    "        movieRecording = TiffImagingInterface )\n",
    "\n",
    "sourceData = dict(\n",
    "      csvIntervals = dict(file_path=csv_file_path),\n",
    "      movieRecording = dict(file_path=movie_path, sampling_frequency=15.0))\n",
    "\n",
    "dual_converter = MyConverter(sourceData)\n",
    "\n",
    "metadata = dual_converter.get_metadata()\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e326706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[\"NWBFile\"][\"session_start_time\"] = session_start_time\n",
    "dual_converter.run_conversion(metadata=metadata, nwbfile_path=prob1b_nwb_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5f6282",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "In this problem we have an NWB file with some data (the file we\n",
    "created in problem 1a) and we have acquired some new data the tiff\n",
    "file from problem 1b). We want to have all the data in a single file.\n",
    "We will use two approaches:\n",
    "1. Append the data to an existing nwb file on disk.\n",
    "2. Create a new nwb file in memory file and save it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504e7188",
   "metadata": {},
   "source": [
    "#### Create an appropriate interface\n",
    "First we create an appropriate interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6853b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_interface = TiffImagingInterface(file_path=movie_path, sampling_frequency=15.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14005d1b",
   "metadata": {},
   "source": [
    "#### Append data to an existing NWB file\n",
    "We copy the file we created in problem 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef16cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a copy of the file we created in  problem 1a\n",
    "import shutil\n",
    "prob2a_nwb_file = data_dir / \"problem_2a.nwb\"\n",
    "\n",
    "shutil.copyfile(prob1a_nwb_file, prob2a_nwb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98376cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_interface.run_conversion(prob2a_nwb_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8269cd",
   "metadata": {},
   "source": [
    "#### Create an NWB file in memory and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob2b_nwb_file = data_dir / \"problem_2b.nwb\"\n",
    "with NWBHDF5IO(prob1a_nwb_file, mode = 'r') as fin, NWBHDF5IO(prob2b_nwb_file, mode = 'w' ) as fout:\n",
    "    prob1a = fin.read() # Read nwb file from prob1a\n",
    "    tiff_interface.add_to_nwbfile(prob1a) # Add the photon information to prob1a, modifies in place\n",
    "    fout.export(fin, nwbfile=prob1a) # Export the new file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d741332",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "In this problem we are looking at the scenario where we have an NWB file already. \n",
    "However, we would like to remove some information and save the result as an NWB file.\n",
    "We will start with the NWB file we created in problem 1b and remove the TwoPhotonSeries\n",
    "from acquisition. Not you can pop items only from LabelledDict items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe7a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = NWBHDF5IO(prob1b_nwb_file, mode = 'r')\n",
    "prob1b = fin.read()\n",
    "prob1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05efc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(prob1b.acquisition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f4fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_photon = prob1b.acquisition.pop('TwoPhotonSeries')\n",
    "prob1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abaf250",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob3_nwb_file = data_dir / \"problem_3.nwb\"\n",
    "with NWBHDF5IO(prob3_nwb_file, mode = 'w' ) as fout:\n",
    "    fout.export(fin, nwbfile=prob1b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ddc9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2acc425",
   "metadata": {},
   "source": [
    "## NWBwidgets\n",
    "A closer look at NWB widgets. We will look at somefile from the DANDI archive. Select DANDI using the radio button. Then select dandiset 4, from the dataset we will look at the nwb file for sub-P27CS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed94778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nwbwidgets.panel import Panel\n",
    "Panel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00200b1",
   "metadata": {},
   "source": [
    "# Writing your own neuroconv interface\n",
    "We will take a look on how to write a simple neuroconv interface.\n",
    "Let's assume we have some TTL signals that we have saved in a matlab file.\n",
    "We would like to create an interface to convert such files to the nwb format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56094d45",
   "metadata": {},
   "source": [
    "## Create a mat file with the data to test our code\n",
    "We will create a mat file with some random data the file will also include\n",
    "a label, and a frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb683d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import savemat\n",
    "data = np.outer(\n",
    "    np.random.choice(a=[0,1], p=[0.8, 0.2],replace=True, size=100), \n",
    "    np.ones(10)).reshape(-1)\n",
    "matdict = {'data': data, 'freq': 1000, 'label':'TTLStrobe'}\n",
    "savemat(data_dir/\"test.mat\", matdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7046dd80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TTLStrobe'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "res = loadmat(data_dir/\"test.mat\")\n",
    "#res['freq'][0][0]\n",
    "#res['label'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e91edb",
   "metadata": {},
   "source": [
    "## Looking at BaseDataInterface\n",
    "You can seee that is an Abstract data class and that we need to overwrite the `add_to_nwbfile`\n",
    "and `__init__` method of the BaseDataInterface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30041b97",
   "metadata": {},
   "source": [
    "```\n",
    "class BaseDataInterface(ABC):\n",
    "    \"\"\"Abstract class defining the structure of all DataInterfaces.\"\"\"\n",
    "\n",
    "    display_name: Union[str, None] = None\n",
    "    keywords: tuple[str] = tuple()\n",
    "    associated_suffixes: tuple[str] = tuple()\n",
    "    info: Union[str, None] = None\n",
    "\n",
    "    @classmethod\n",
    "    def get_source_schema(cls) -> dict:\n",
    "        \"\"\"Infer the JSON schema for the source_data from the method signature (annotation typing).\"\"\"\n",
    "        return get_json_schema_from_method_signature(cls, exclude=[\"source_data\"])\n",
    "\n",
    "    @classmethod\n",
    "    def validate_source(cls, source_data: dict, verbose: bool = False):\n",
    "        \"\"\"Validate source_data against Converter source_schema.\"\"\"\n",
    "        cls._validate_source_data(source_data=source_data, verbose=verbose)\n",
    "\n",
    "    def _validate_source_data(self, source_data: dict, verbose: bool = False):\n",
    "\n",
    "        encoder = _NWBSourceDataEncoder()\n",
    "        # The encoder produces a serialized object, so we deserialized it for comparison\n",
    "\n",
    "        serialized_source_data = encoder.encode(source_data)\n",
    "        decoded_source_data = json.loads(serialized_source_data)\n",
    "        source_schema = self.get_source_schema()\n",
    "        validate(instance=decoded_source_data, schema=source_schema)\n",
    "        if verbose:\n",
    "            print(\"Source data is valid!\")\n",
    "\n",
    "    @validate_call\n",
    "    def __init__(self, verbose: bool = False, **source_data):\n",
    "        self.verbose = verbose\n",
    "        self.source_data = source_data\n",
    "\n",
    "        self._validate_source_data(source_data=source_data, verbose=verbose)\n",
    "\n",
    "    def get_metadata_schema(self) -> dict:\n",
    "        \"\"\"Retrieve JSON schema for metadata.\"\"\"\n",
    "        metadata_schema = load_dict_from_file(Path(__file__).parent / \"schemas\" / \"base_metadata_schema.json\")\n",
    "        return metadata_schema\n",
    "\n",
    "    def get_metadata(self) -> DeepDict:\n",
    "        \"\"\"Child DataInterface classes should override this to match their metadata.\"\"\"\n",
    "        metadata = DeepDict()\n",
    "        metadata[\"NWBFile\"][\"session_description\"] = \"\"\n",
    "        metadata[\"NWBFile\"][\"identifier\"] = str(uuid.uuid4())\n",
    "\n",
    "        # Add NeuroConv watermark (overridden if going through the GUIDE)\n",
    "        neuroconv_version = importlib.metadata.version(\"neuroconv\")\n",
    "        metadata[\"NWBFile\"][\"source_script\"] = f\"Created using NeuroConv v{neuroconv_version}\"\n",
    "        metadata[\"NWBFile\"][\"source_script_file_name\"] = __file__  # Required for validation\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    def validate_metadata(self, metadata: dict, append_mode: bool = False) -> None:\n",
    "        \"\"\"Validate the metadata against the schema.\"\"\"\n",
    "        encoder = _NWBMetaDataEncoder()\n",
    "        # The encoder produces a serialized object, so we deserialized it for comparison\n",
    "\n",
    "        serialized_metadata = encoder.encode(metadata)\n",
    "        decoded_metadata = json.loads(serialized_metadata)\n",
    "        metdata_schema = self.get_metadata_schema()\n",
    "        if append_mode:\n",
    "            # Eliminate required from NWBFile\n",
    "            nwbfile_schema = metdata_schema[\"properties\"][\"NWBFile\"]\n",
    "            nwbfile_schema.pop(\"required\", None)\n",
    "\n",
    "        validate(instance=decoded_metadata, schema=metdata_schema)\n",
    "\n",
    "    def get_conversion_options_schema(self) -> dict:\n",
    "        \"\"\"Infer the JSON schema for the conversion options from the method signature (annotation typing).\"\"\"\n",
    "        return get_json_schema_from_method_signature(self.add_to_nwbfile, exclude=[\"nwbfile\", \"metadata\"])\n",
    "\n",
    "    def create_nwbfile(self, metadata: Optional[dict] = None, **conversion_options) -> NWBFile:\n",
    "        \"\"\"\n",
    "        Create and return an in-memory pynwb.NWBFile object with this interface's data added to it.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        metadata : dict, optional\n",
    "            Metadata dictionary with information used to create the NWBFile.\n",
    "        **conversion_options\n",
    "            Additional keyword arguments to pass to the `.add_to_nwbfile` method.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        nwbfile : pynwb.NWBFile\n",
    "            The in-memory object with this interface's data added to it.\n",
    "        \"\"\"\n",
    "        if metadata is None:\n",
    "            metadata = self.get_metadata()\n",
    "\n",
    "        nwbfile = make_nwbfile_from_metadata(metadata=metadata)\n",
    "        self.add_to_nwbfile(nwbfile=nwbfile, metadata=metadata, **conversion_options)\n",
    "\n",
    "        return nwbfile\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_to_nwbfile(self, nwbfile: NWBFile, **conversion_options) -> None:\n",
    "        \"\"\"\n",
    "        Define a protocol for mapping the data from this interface to NWB neurodata objects.\n",
    "\n",
    "        These neurodata objects should also be added to the in-memory pynwb.NWBFile object in this step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nwbfile : pynwb.NWBFile\n",
    "            The in-memory object to add the data to.\n",
    "        **conversion_options\n",
    "            Additional keyword arguments to pass to the `.add_to_nwbfile` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def run_conversion(\n",
    "        self,\n",
    "        nwbfile_path: FilePath,\n",
    "        nwbfile: Optional[NWBFile] = None,\n",
    "        metadata: Optional[dict] = None,\n",
    "        overwrite: bool = False,\n",
    "        backend: Optional[Literal[\"hdf5\", \"zarr\"]] = None,\n",
    "        backend_configuration: Optional[Union[HDF5BackendConfiguration, ZarrBackendConfiguration]] = None,\n",
    "        **conversion_options,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run the NWB conversion for the instantiated data interface.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nwbfile_path : FilePathType\n",
    "            Path for where the data will be written or appended.\n",
    "        nwbfile : NWBFile, optional\n",
    "            An in-memory NWBFile object to write to the location.\n",
    "        metadata : dict, optional\n",
    "            Metadata dictionary with information used to create the NWBFile when one does not exist or overwrite=True.\n",
    "        overwrite : bool, default: False\n",
    "            Whether to overwrite the NWBFile if one exists at the nwbfile_path.\n",
    "            The default is False (append mode).\n",
    "        backend : {\"hdf5\", \"zarr\"}, optional\n",
    "            The type of backend to use when writing the file.\n",
    "            If a `backend_configuration` is not specified, the default type will be \"hdf5\".\n",
    "            If a `backend_configuration` is specified, then the type will be auto-detected.\n",
    "        backend_configuration : HDF5BackendConfiguration or ZarrBackendConfiguration, optional\n",
    "            The configuration model to use when configuring the datasets for this backend.\n",
    "            To customize, call the `.get_default_backend_configuration(...)` method, modify the returned\n",
    "            BackendConfiguration object, and pass that instead.\n",
    "            Otherwise, all datasets will use default configuration settings.\n",
    "        \"\"\"\n",
    "\n",
    "        backend = _resolve_backend(backend, backend_configuration)\n",
    "        no_nwbfile_provided = nwbfile is None  # Otherwise, variable reference may mutate later on inside the context\n",
    "\n",
    "        if metadata is None:\n",
    "            metadata = self.get_metadata()\n",
    "\n",
    "        file_initially_exists = Path(nwbfile_path).exists() if nwbfile_path is not None else False\n",
    "        append_mode = file_initially_exists and not overwrite\n",
    "\n",
    "        self.validate_metadata(metadata=metadata, append_mode=append_mode)\n",
    "\n",
    "        with make_or_load_nwbfile(\n",
    "            nwbfile_path=nwbfile_path,\n",
    "            nwbfile=nwbfile,\n",
    "            metadata=metadata,\n",
    "            overwrite=overwrite,\n",
    "            backend=backend,\n",
    "            verbose=getattr(self, \"verbose\", False),\n",
    "        ) as nwbfile_out:\n",
    "            if no_nwbfile_provided:\n",
    "                self.add_to_nwbfile(nwbfile=nwbfile_out, metadata=metadata, **conversion_options)\n",
    "\n",
    "            if backend_configuration is None:\n",
    "                backend_configuration = self.get_default_backend_configuration(nwbfile=nwbfile_out, backend=backend)\n",
    "\n",
    "            configure_backend(nwbfile=nwbfile_out, backend_configuration=backend_configuration)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_backend_configuration(\n",
    "        nwbfile: NWBFile,\n",
    "        # TODO: when all H5DataIO prewraps are gone, introduce Zarr safely\n",
    "        # backend: Union[Literal[\"hdf5\", \"zarr\"]],\n",
    "        backend: Literal[\"hdf5\"] = \"hdf5\",\n",
    "    ) -> Union[HDF5BackendConfiguration, ZarrBackendConfiguration]:\n",
    "        \"\"\"\n",
    "        Fill and return a default backend configuration to serve as a starting point for further customization.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nwbfile : pynwb.NWBFile\n",
    "            The in-memory object with this interface's data already added to it.\n",
    "        backend : \"hdf5\", default: \"hdf5\"\n",
    "            The type of backend to use when creating the file.\n",
    "            Additional backend types will be added soon.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        backend_configuration : HDF5BackendConfiguration or ZarrBackendConfiguration\n",
    "            The default configuration for the specified backend type.\n",
    "        \"\"\"\n",
    "        return get_default_backend_configuration(nwbfile=nwbfile, backend=backend)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888e6106",
   "metadata": {},
   "source": [
    "## Imports \n",
    "We will use the following imports in constructing our class.\n",
    "The notebook format is not really appropriate for creating \n",
    "a class, this is something you would likely want to do\n",
    "as a python module. You are only using the notebook presentation\n",
    "for ease of use and convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0815800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from neuroconv import BaseDataInterface\n",
    "from pydantic import FilePath\n",
    "from pydantic.validate_call_decorator import validate_call\n",
    "from scipy.io import loadmat\n",
    "from pynwb import NWBFile, TimeSeries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd8e2ac",
   "metadata": {},
   "source": [
    "## Extend base data interface class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "950085ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatTTL(BaseDataInterface):\n",
    "    \"\"\" My class to convert matlab files to TLL\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1435fb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class MatTTL with abstract method add_to_nwbfile",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mMatTTL\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class MatTTL with abstract method add_to_nwbfile"
     ]
    }
   ],
   "source": [
    "MatTTL(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "331bc4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatTTL(BaseDataInterface):\n",
    "    \"\"\" My class to convert matlab files to TLL\"\"\"\n",
    "    \n",
    "\n",
    "    def add_to_nwbfile(self, nwbfile: NWBFile, metadata: Optional[dict], **conversion_options) -> None:\n",
    "        \"\"\"\n",
    "        Define a protocol for mapping the data from this interface to NWB neurodata objects.\n",
    "\n",
    "        These neurodata objects should also be added to the in-memory pynwb.NWBFile object in this step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nwbfile : pynwb.NWBFile\n",
    "            The in-memory object to add the data to.\n",
    "        **conversion_options\n",
    "            Additional keyword arguments to pass to the `.add_to_nwbfile` method.\n",
    "        \"\"\"\n",
    "        ts = TimeSeries(name=self.name, \n",
    "                        data=self.data, \n",
    "                        unit=\"V\", \n",
    "                        starting_time=self.starting_time, \n",
    "                        rate= self.rate)\n",
    "        nwbfile.add_acquisition(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a9f37af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data is valid!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MatTTL at 0x7f50d4bef650>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MatTTL(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea3b7fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MatTTL(BaseDataInterface):\n",
    "    \"\"\" My class to convert matlab files to TTL \"\"\"\n",
    "    @validate_call\n",
    "    def __init__(self,\n",
    "                 file_path: FilePath,\n",
    "                 verbose: bool = True\n",
    "                 ):\n",
    "        super().__init__(verbose,file_path=file_path)\n",
    "        res = loadmat(file_path) # Read matlab file\n",
    "        self.starting_time = 0.0 # Assume that starting time is alway the start time of the session\n",
    "        self.name = res.get('label', ['TTLSignal'])[0]\n",
    "        self.rate = float(res.get('freq', [[1000]])[0][0])\n",
    "        self.data = res.get('data')\n",
    "\n",
    "    def add_to_nwbfile(self, nwbfile: NWBFile, metadata: Optional[dict], **conversion_options) -> None:\n",
    "        ts = TimeSeries(name=self.name, \n",
    "                        data=self.data, \n",
    "                        unit=\"V\", \n",
    "                        starting_time=self.starting_time, \n",
    "                        rate= self.rate)\n",
    "        nwbfile.add_acquisition(ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e7e0836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data is valid!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MatTTL at 0x7f50d514d9d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MatTTL(file_path=data_dir/\"test.mat\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9f01b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data is valid!\n",
      "NWB file saved at ../data/test.nwb!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "mat_file = data_dir/\"test.mat\"\n",
    "mat_nwb_file = data_dir /\"test.nwb\"\n",
    "mat_interface = MatTTL(file_path=mat_file, verbose=True)\n",
    "metadata = mat_interface.get_metadata()\n",
    "metadata['NWBFile']['session_start_time'] = datetime.datetime.now(tz=ZoneInfo(\"Europe/Paris\"))\n",
    "mat_interface.run_conversion(mat_nwb_file, metadata= metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f58eeaa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .container-fields {\n",
       "                font-family: \"Open Sans\", Arial, sans-serif;\n",
       "            }\n",
       "            .container-fields .field-value {\n",
       "                color: #00788E;\n",
       "            }\n",
       "            .container-fields details > summary {\n",
       "                cursor: pointer;\n",
       "                display: list-item;\n",
       "            }\n",
       "            .container-fields details > summary:hover {\n",
       "                color: #0A6EAA;\n",
       "            }\n",
       "        </style>\n",
       "        \n",
       "        <script>\n",
       "            function copyToClipboard(text) {\n",
       "                navigator.clipboard.writeText(text).then(function() {\n",
       "                    console.log('Copied to clipboard: ' + text);\n",
       "                }, function(err) {\n",
       "                    console.error('Could not copy text: ', err);\n",
       "                });\n",
       "            }\n",
       "\n",
       "            document.addEventListener('DOMContentLoaded', function() {\n",
       "                let fieldKeys = document.querySelectorAll('.container-fields .field-key');\n",
       "                fieldKeys.forEach(function(fieldKey) {\n",
       "                    fieldKey.addEventListener('click', function() {\n",
       "                        let accessCode = fieldKey.getAttribute('title').replace('Access code: ', '');\n",
       "                        copyToClipboard(accessCode);\n",
       "                    });\n",
       "                });\n",
       "            });\n",
       "        </script>\n",
       "        <div class='container-wrap'><div class='container-header'><div class='xr-obj-type'><h3>root (NWBFile)</h3></div></div><div style=\"margin-left: 0px;\" class=\"container-fields\"><span class=\"field-key\" title=\".session_description\">session_description: </span><span class=\"field-value\"></span></div><div style=\"margin-left: 0px;\" class=\"container-fields\"><span class=\"field-key\" title=\".identifier\">identifier: </span><span class=\"field-value\">762e22b5-7830-4f18-b016-ae0b1c6742bb</span></div><details><summary style=\"display: list-item; margin-left: 0px;\" class=\"container-fields field-key\" title=\".session_start_time\"><b>session_start_time</b></summary><span class=\"field-key\">2025-01-20 14:42:05.444219+01:00</span></details><details><summary style=\"display: list-item; margin-left: 0px;\" class=\"container-fields field-key\" title=\".timestamps_reference_time\"><b>timestamps_reference_time</b></summary><span class=\"field-key\">2025-01-20 14:42:05.444219+01:00</span></details><details><summary style=\"display: list-item; margin-left: 0px;\" class=\"container-fields field-key\" title=\".file_create_date\"><b>file_create_date</b></summary><details><summary style=\"display: list-item; margin-left: 20px;\" class=\"container-fields field-key\" title=\".file_create_date[0]\"><b>0</b></summary><span class=\"field-key\">2025-01-20 14:42:05.458887+01:00</span></details></details><details><summary style=\"display: list-item; margin-left: 0px;\" class=\"container-fields field-key\" title=\".acquisition\"><b>acquisition</b></summary><details><summary style=\"display: list-item; margin-left: 20px;\" class=\"container-fields field-key\" title=\".acquisition['TTLStrobe']\"><b>TTLStrobe</b></summary><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".acquisition['TTLStrobe'].starting_time\">starting_time: </span><span class=\"field-value\">0.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".acquisition['TTLStrobe'].rate\">rate: </span><span class=\"field-value\">1000.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".acquisition['TTLStrobe'].resolution\">resolution: </span><span class=\"field-value\">-1.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".acquisition['TTLStrobe'].comments\">comments: </span><span class=\"field-value\">no comments</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".acquisition['TTLStrobe'].description\">description: </span><span class=\"field-value\">no description</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".acquisition['TTLStrobe'].conversion\">conversion: </span><span class=\"field-value\">1.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".acquisition['TTLStrobe'].offset\">offset: </span><span class=\"field-value\">0.0</span></div><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".acquisition['TTLStrobe'].unit\">unit: </span><span class=\"field-value\">V</span></div><details><summary style=\"display: list-item; margin-left: 40px;\" class=\"container-fields field-key\" title=\".acquisition['TTLStrobe'].data\"><b>data</b></summary><div style=\"margin-left: 60px;\" class=\"container-fields\">HDF5 dataset<br><table class=\"data-info\"><tbody><tr><th style=\"text-align: left\">Data type</th><td style=\"text-align: left\">float64</td></tr><tr><th style=\"text-align: left\">Shape</th><td style=\"text-align: left\">(1, 1000)</td></tr><tr><th style=\"text-align: left\">Array size</th><td style=\"text-align: left\">7.81 KiB</td></tr><tr><th style=\"text-align: left\">Chunk shape</th><td style=\"text-align: left\">(1, 1000)</td></tr><tr><th style=\"text-align: left\">Compression</th><td style=\"text-align: left\">gzip</td></tr><tr><th style=\"text-align: left\">Compression opts</th><td style=\"text-align: left\">4</td></tr><tr><th style=\"text-align: left\">Compression ratio</th><td style=\"text-align: left\">60.60606060606061</td></tr></tbody></table></div></details><div style=\"margin-left: 40px;\" class=\"container-fields\"><span class=\"field-key\" title=\".acquisition['TTLStrobe'].starting_time_unit\">starting_time_unit: </span><span class=\"field-value\">seconds</span></div></details></details><div style=\"margin-left: 0px;\" class=\"container-fields\"><span class=\"field-key\" title=\".source_script\">source_script: </span><span class=\"field-value\">Created using NeuroConv v0.6.6</span></div><div style=\"margin-left: 0px;\" class=\"container-fields\"><span class=\"field-key\" title=\".source_script_file_name\">source_script_file_name: </span><span class=\"field-value\">/home/apitsill/ephemera/nwb_workshops/lib/python3.11/site-packages/neuroconv/basedatainterface.py</span></div></div>"
      ],
      "text/plain": [
       "root pynwb.file.NWBFile at 0x139985143198288\n",
       "Fields:\n",
       "  acquisition: {\n",
       "    TTLStrobe <class 'pynwb.base.TimeSeries'>\n",
       "  }\n",
       "  file_create_date: [datetime.datetime(2025, 1, 20, 14, 42, 5, 458887, tzinfo=tzoffset(None, 3600))]\n",
       "  identifier: 762e22b5-7830-4f18-b016-ae0b1c6742bb\n",
       "  session_start_time: 2025-01-20 14:42:05.444219+01:00\n",
       "  source_script: Created using NeuroConv v0.6.6\n",
       "  source_script_file_name: /home/apitsill/ephemera/nwb_workshops/lib/python3.11/site-packages/neuroconv/basedatainterface.py\n",
       "  timestamps_reference_time: 2025-01-20 14:42:05.444219+01:00"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin = NWBHDF5IO(mat_nwb_file, mode = 'r')\n",
    "mat_nwb = fin.read()\n",
    "mat_nwb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ca5ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nwb_workshops",
   "language": "python",
   "name": "nwb_workshops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
