{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f52409c6",
   "metadata": {},
   "source": [
    "# Pre requisites\n",
    "\n",
    "If you were running this on your own machine you would have to install jupyter, neuroconv package and the data.\n",
    "\n",
    "However, if you are using either binder or github codespaces for the workshop these steps have already been done for you so you don't need to install anything.\n",
    "\n",
    "\n",
    "1- Download, install and open GUIDE : https://nwb-guide.readthedocs.io/en/stable/\n",
    "\n",
    "2- Get datasets on your local machine : \n",
    "\n",
    "Option 1 for IDA members: Connect to Gaia/@ida/Equipements_Materials_and_Platforms/PLATEFORMES/PF_Signal/NWB_Workshop2025\n",
    "\n",
    "Option 2 for Pasteur members : Download 3 NWB datasets on Pasteur Drive (Total 39Go)\n",
    "\n",
    "https://drive.pasteur.fr/s/Jp4ibFHEQ8rz2Km\n",
    "\n",
    "https://drive.pasteur.fr/s/5kSEqaE6mqPtSnH \n",
    "        \n",
    "https://drive.pasteur.fr/s/iLGB2FH6wSd9kpD\n",
    "\n",
    "with the following password: ILoveNWB@2025\n",
    "\n",
    "\n",
    "# NWB workshop\n",
    "In this section, you will load NWB datasets with the no-code tool \"GUIDE\".\n",
    "\n",
    "## Dataset 1 : BASIL acquisition \n",
    "\n",
    "1- In the GUIDE app, click Explore, load and find in your local repository the file sub-390_ses-17.nwb\n",
    "\n",
    "2- Expand the « acquisition » Tab. \n",
    "\n",
    "3- Check the « Lick » and click the View 1 item. Zoom to 10s of experiments. \n",
    "\n",
    "4- Close and now check 3 of these items : Lick, Reward, TTLTrigcamera1, TTLTrigsound, TrialType. Zoom on 3 trials.\n",
    "\n",
    "5- Close and click « open in pynwb ». With few line of codes, you can open and visualize the data in a notebook as follow: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2aa79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pathlib import Path\n",
    "from pynwb import NWBHDF5IO\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a55cb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"../data\") # Data path\n",
    "data_dir.mkdir(exist_ok=True) # Create data dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456e2c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download BASIL dataset on the binder workspace\n",
    "if [[ ! -e \"../data/sub-390_ses-17.nwb\" ]]; then\n",
    "   wget --user \"5kSEqaE6mqPtSnH\" --password \"ILoveNWB@2025\" \"https://drive.pasteur.fr/public.php/webdav\" -O ../data/sub-390_ses-17.nwb\n",
    "fi        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70568e41-7a3a-44fb-9bcf-d485bc4f1d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset, look at the data structure and plot timeseries\n",
    "folder_path = \"Y:/Equipements_Materials_and_Platforms/PLATEFORMES/PF_Signal/NWB_Workshop2025/sub-390_ses-17.nwb\"\n",
    "#folder_path = \"../data/sub-390_ses-17.nwb\"\n",
    "io = NWBHDF5IO(folder_path, mode='r')\n",
    "nwbfile = io.read()\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80c083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize timeseries\n",
    "plt.plot(nwbfile.acquisition['Lick'].data[100:20000])\n",
    "plt.plot(nwbfile.acquisition['TTLtrigsounds'].data[100:20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b282ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7b3bb2",
   "metadata": {},
   "source": [
    "## Dataset 2 : Ephys acquisition\n",
    "\n",
    "Here, we explore a dataset including : \n",
    "\n",
    "- Raw electrophysiology data acquired with spikeglx \n",
    "\n",
    "- Preprocessing / spike sorting performed with Kilosort\n",
    "\n",
    "- Behavior/stimuli data acquired with BASIL.\n",
    "\n",
    "Dataset courtesy : Pierre Platel, DSAPM.\n",
    "\n",
    "In GUIDE app :\n",
    "\n",
    "1- In the GUIDE app, click Explore, load and find in your local repository the file spikeglx_kilosort_BASIL_NWBfile.nwb \n",
    "\n",
    "2- Expand the «unit» Tab and have a look at the units description. \n",
    "\n",
    "3- Check the raster plot and click « view 1 item ». \n",
    "\n",
    "Zoom on 10s\n",
    "\n",
    "Click « view more units »\n",
    "\n",
    "4- Close and click « open in pynwb ». With few line of codes, you can open and visualize the data in a notebook as follow. \n",
    "\n",
    "For storage issue, you can not download the data on binder. We provide the line of codes to do so but it will make your notebook crash. If interested, you can create a local python environment and copy these lines of code to vizualise and process your data as follow.\n",
    "\n",
    "### Look at the data structure and plot spike raster plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf78719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "## Download ephys and spike sorting dataset\n",
    "#if [[ ! -e \"../data/spikeglx_kilosort_BASIL_NWBfile.nwb\" ]]; then\n",
    "#    wget --user \"Jp4ibFHEQ8rz2Km\" --password \"ILoveNWB@2025\" \"https://drive.pasteur.fr/public.php/webdav\" -O ../data/spikeglx_kilosort_BASIL_NWBfile.nwb\n",
    "#fi            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b6617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder_path = \"../data/spikeglx_kilosort_BASIL_NWBfile.nwb\"\n",
    "folder_path = \"Y:/Equipements_Materials_and_Platforms/PLATEFORMES/PF_Signal/NWB_Workshop2025/spikeglx_kilosort_BASIL_NWBfile.nwb\"\n",
    "io = NWBHDF5IO(folder_path, mode='r')\n",
    "nwbfile = io.read()\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9fd1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "units = nwbfile.units\n",
    "units_spike_times = units[\"spike_times\"]\n",
    "# bin size for counting spikes\n",
    "time_resolution = 0.01\n",
    "\n",
    "# start and end times (relative to the stimulus at 0 seconds) that we want to examine and align spikes to\n",
    "window_start_time = -0.1\n",
    "window_end_time = 2\n",
    "\n",
    "# time bins used\n",
    "n_bins = int((window_end_time - window_start_time) / time_resolution)\n",
    "bin_edges = np.linspace(window_start_time, window_end_time, n_bins, endpoint=True)\n",
    "\n",
    "# useful throughout analysis\n",
    "n_units = len(units_spike_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4987993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 1\n",
    "\n",
    "# 3D spike matrix to be populated with spike counts\n",
    "spike_matrix = np.zeros((n_units, len(bin_edges), n_trials))\n",
    "\n",
    "# populate 3D spike matrix for each unit for each stimulus trial by counting spikes into bins\n",
    "for unit_idx in range(n_units):\n",
    "    spike_times = units_spike_times[unit_idx]\n",
    "    \n",
    "    # get spike times that fall within the bin's time range relative to the stim time        \n",
    "    first_bin_time = bin_edges[0]\n",
    "    last_bin_time = bin_edges[-1]\n",
    "    first_spike_in_range, last_spike_in_range = np.searchsorted(spike_times, [first_bin_time, last_bin_time])\n",
    "    spike_times_in_range = spike_times[first_spike_in_range:last_spike_in_range]\n",
    "\n",
    "    # convert spike times into relative time bin indices\n",
    "    bin_indices = ((spike_times_in_range - (first_bin_time)) / time_resolution).astype(int)\n",
    "    \n",
    "    # mark that there is a spike at these bin times for this unit on this stim trial\n",
    "    for bin_idx in bin_indices:\n",
    "        spike_matrix[unit_idx, bin_idx, 0] += 1\n",
    "\n",
    "spike_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d9dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "trial = 0\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "ax.set_title(\"Unit Spikes\")\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax.set_ylabel(\"Unit #\")\n",
    "\n",
    "img = ax.imshow(spike_matrix[:,:,trial], extent=[window_start_time,window_end_time,0,n_units], aspect=0.001, vmin=0, vmax=1)\n",
    "cbar = fig.colorbar(img, shrink=0.5)\n",
    "cbar.set_label(\"# Spikes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46f040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d7228",
   "metadata": {},
   "source": [
    "## Dataset 3 : 2P Ca2+ imaging acquisition\n",
    "\n",
    "Here, we explore a dataset including : \n",
    "\n",
    "- Preprocessing : segmentation and fluorescence trace performed with Suite2P on data acquired with a 2ph microscope (Karthala or Mega2P, at the Hearing Institute)\n",
    "\n",
    "Note, we could also have included BASIL acquisition to visualize sound stimuli and raw calcium imaging stack file in the same NWB file.\n",
    "\n",
    "Dataset courtesy : Amel Saoudi (TGTD) and Anthony Lourdiane (DSAPM)\n",
    "\n",
    "In GUIDE app :\n",
    "\n",
    "1- In the GUIDE app, click Explore, load and find in your local repository the file Karthala_Suite2P_NWBfile.nwb\n",
    "\n",
    "2- Expand the «processing/ophys» Tab.\n",
    "\n",
    "3- Check the « ImageSegmentation » and click « view 1 item ». \n",
    "\n",
    "Look at cell segmentation of each single plane (4 in total)\n",
    "\n",
    "4- Uncheck « Image segmentation » and check « Segmentation Images ».\n",
    "\n",
    "Click view 1 item and visualize correlation and mean images.\n",
    "\n",
    "5- Close, Uncheck and now check Fluorescence/RoiResponseSeriesChan1Plane0\n",
    "\n",
    "Click View 1 item\n",
    "\n",
    "Choose  # visible chans: 10.\n",
    "\n",
    "You can also zoom on a smaller time window\n",
    "\n",
    "6- Close and click « open in pynwb ». With few line of codes, you can open and visualize the data in a notebook as follow. \n",
    "\n",
    "For storage issue, you can not download the data on binder. We provide the line of codes to do so but it will make your notebook crash. If interested, you can create a local python environment and copy these lines of code to vizualise and process your data as follow.\n",
    "\n",
    "### Look at the data structure and plot calcium imaging raster plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2add2b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "## Download Calcium imaging and Suite2P dataset\n",
    "#if [[ ! -e \"../data/Karthala_Suite2P_NWBfile.nwb\" ]]; then\n",
    "#    wget --user \"q6m5SXzmmA8fZwD\" --password \"ILoveNWB@2025\" \"https://drive.pasteur.fr/public.php/webdav\" -O ../data/Karthala_Suite2P_NWBfile.nwb\n",
    "#fi            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5862f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder_path = \"../data/Karthala_Suite2P_NWBfile.nwb\"\n",
    "folder_path = \"Y:/Equipements_Materials_and_Platforms/PLATEFORMES/PF_Signal/NWB_Workshop2025/Karthala_Suite2P_NWBfile.nwb\"\n",
    "io = NWBHDF5IO(folder_path, mode='r')\n",
    "nwbfile = io.read()\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fb1346",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nwbfile.processing['ophys']\n",
    "Y = X.data_interfaces['Fluorescence']\n",
    "Z = Y.roi_response_series['RoiResponseSeriesChan1Plane0']\n",
    "starting_time = Z.starting_time[()]\n",
    "rate = Z.rate\n",
    "data = Z.data\n",
    "n_cells = data.shape[1]\n",
    "\n",
    "print(f'starting_time: {starting_time}')\n",
    "print(f'rate: {rate}')\n",
    "print(f'data shape: {data.shape}')\n",
    "print(f'n cells: {n_cells}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b24414",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "ax.set_title(\"Raw fluorescence trace\")\n",
    "ax.set_xlabel(\"Time (frame)\")\n",
    "ax.set_ylabel(\"Cells #\")\n",
    "\n",
    "img = ax.imshow(np.transpose(data[0:50000,1:50]), aspect=500)\n",
    "cbar.set_label(\"# Spikes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeda423",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba595a44-09b4-4886-9ebd-e2e42c812a40",
   "metadata": {},
   "source": [
    "## Converting to NWB with neuroconv\n",
    "\n",
    "We will deal with three problems that we think arise often.\n",
    "The first one is having a type of data that you would like\n",
    "to convert to NWB format and for which there is already a converter in [<img src=https://neuroconv.readthedocs.io/en/main/_images/neuroconv_logo.png width=50 height=50>](https://neuroconv.readthedocs.io/en/main/) The available converters can be found\n",
    "in the neuroconv [gallery](https://neuroconv.readthedocs.io/en/main/conversion_examples_gallery/index.html).\n",
    "The second one is already\n",
    "having an NWB file and some data, for which there is a neuroconv datainterface, that you would like to add to the NWB file. The third problem is having an NWB file from which you would like to extract some data and create a new file.\n",
    "\n",
    "\n",
    "### Problem 1a:\n",
    "We will start with the simplest case scenario.\n",
    "We have a single source of data. The data that we will use are from a csv file, \n",
    "and to convert them to NWB we will use the [`CsvTimeIntervalsInterface`](https://neuroconv.readthedocs.io/en/main/conversion_examples_gallery/text/csv.html) from neuroconv.\n",
    "The procedure will be the same for any type of data and the corresponding neuroconv interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1163d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requirement for the conversion\n",
    "from neuroconv.datainterfaces import CsvTimeIntervalsInterface\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a6074",
   "metadata": {},
   "source": [
    "#### Create CSV file if not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91454c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "data_dir = Path(\"../data\") # Data path\n",
    "data_dir.mkdir(exist_ok=True) # Create data dir\n",
    "csv_file_path = data_dir / \"mydata.csv\" # Form file data path\n",
    "random.seed(42) # set seed\n",
    "i = 0\n",
    "starts  = []\n",
    "ends  = []\n",
    "vals = []\n",
    "s0 = 0\n",
    "shift = 0.00001\n",
    "while (not ends) or ends[-1] < 120 :\n",
    "    starts.append(s0 + random.randrange(1,4))\n",
    "    end = starts[-1] + random.randrange(1,8)\n",
    "    vals.append(random.uniform(0,1))\n",
    "    s0 = end\n",
    "    ends.append(end)\n",
    "trial_times = pd.DataFrame({'start_time':starts, 'end_time':ends, 'value':vals })\n",
    "# Write csv file if it doesn't exist\n",
    "if not csv_file_path.exists():\n",
    "    trial_times.to_csv(csv_file_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014fc4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show first 5 content lines of csv file\n",
    "pd.read_csv(csv_file_path).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8440de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show first 5 content lines of csv file with bash\n",
    "### 1 line for the header + 5 lines for the contents\n",
    "!head -n6 ../data/mydata.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb71c1b",
   "metadata": {},
   "source": [
    "#### Create interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc721eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_interface = CsvTimeIntervalsInterface(file_path=csv_file_path, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dab7a04",
   "metadata": {},
   "source": [
    "#### Get metadata from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb97626",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = csv_interface.get_metadata()\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837e6d6",
   "metadata": {},
   "source": [
    "#### Attempt to create NWB file\n",
    "This is expected to Fail we just want to get an idea of the error messages and how to interpret them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8730f4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob1a_nwb_file = data_dir/\"problem_1a.nwb\"\n",
    "csv_interface.run_conversion(nwbfile_path = prob1a_nwb_file, metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03926ec9",
   "metadata": {},
   "source": [
    "#### Add missing metadata\n",
    "From the error message above we see that we are missing the session_start_time which is a required property.\n",
    "We will add that information in the metadata dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceac4983",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_start_time = datetime(2025,1,10,11,45,0, tzinfo=ZoneInfo(\"Europe/Paris\"))\n",
    "metadata[\"NWBFile\"][\"session_start_time\"] = session_start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e88b33",
   "metadata": {},
   "source": [
    "#### Create NWB file\n",
    "Now that we have have the required metadata we can try again to create the NWB file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a227e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_interface.run_conversion(nwbfile_path = prob1a_nwb_file, metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d317de",
   "metadata": {},
   "source": [
    "#### Look at the generated NWB file\n",
    "##### Use the pynwb library to read the file\n",
    "First we look at the file using the pynwb library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888c30d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "import pynwb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcf5874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideally we would use a with statement \n",
    "# as in the commented code\n",
    "#\n",
    "## with NWBHDF5IO(prob1a_nwb_file, mode = 'r') as io:\n",
    "##    nwbfile = io.read()\n",
    "##    nwbfile\n",
    "# we instead use the code below to be able to\n",
    "# see a nice version of the file.\n",
    "io = NWBHDF5IO(prob1a_nwb_file, mode = 'r')\n",
    "nwbfile = io.read()\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7407341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close() # Don't forget to close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f655e3b",
   "metadata": {},
   "source": [
    "##### Use NWBwidgets to read the file.\n",
    "Just a taste of how NWBwidgets work, we will take a closer look at NWBwidgets later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ca270f-a280-471d-831a-476805bc876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nwbwidgets import nwb2widget\n",
    "\n",
    "io = NWBHDF5IO(prob1a_nwb_file, mode = 'r')\n",
    "nwbfile = io.read()\n",
    "nwb2widget(nwbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0e1f9-6e7d-44c2-8110-4dfe883120e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee634be7",
   "metadata": {},
   "source": [
    "##### Use h5py to read the file\n",
    "Since our file is an HDF5 file we use any of the HDF5 libraries to read the file. Here we are using the h5py library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24357933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File(prob1a_nwb_file, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d26b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3dec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_intervals = f.get('intervals')\n",
    "my_print = lambda x,_: print(x)\n",
    "my_intervals.visititems(my_print)\n",
    "print(my_intervals.get('trials').get('value')[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a12d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ec48c3",
   "metadata": {},
   "source": [
    "###  Problem 1b\n",
    "Now we again want to generate a NWBFile from already available data.\n",
    "However, in addition to the csv file with some tiff images that \n",
    "should also be inside the generated NWB file.\n",
    "#### Download tiff file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017979b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download movie file if not already available\n",
    "if [[ ! -e \"../data/demoMovie.tif\" ]]; then\n",
    "   wget https://github.com/flatironinstitute/CaImAn/raw/refs/heads/main/example_movies/demoMovie.tif -O ../data/demoMovie.tif\n",
    "fi        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1d7b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroconv.datainterfaces import TiffImagingInterface\n",
    "from neuroconv import NWBConverter\n",
    "movie_path = data_dir / 'demoMovie.tif'\n",
    "prob1b_nwb_file = data_dir / 'problem_1b.nwb'\n",
    "\n",
    "\n",
    "class MyConverter(NWBConverter):\n",
    "    data_interface_classes = dict (\n",
    "        csvIntervals = CsvTimeIntervalsInterface,\n",
    "        movieRecording = TiffImagingInterface )\n",
    "\n",
    "sourceData = dict(\n",
    "      csvIntervals = dict(file_path=csv_file_path),\n",
    "      movieRecording = dict(file_path=movie_path, sampling_frequency=15.0))\n",
    "\n",
    "dual_converter = MyConverter(sourceData)\n",
    "\n",
    "metadata = dual_converter.get_metadata()\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e326706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[\"NWBFile\"][\"session_start_time\"] = session_start_time\n",
    "dual_converter.run_conversion(metadata=metadata, nwbfile_path=prob1b_nwb_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92153b91",
   "metadata": {},
   "source": [
    "#### Look at the generated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f455c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "io = NWBHDF5IO(prob1b_nwb_file, mode='r') \n",
    "nwbfile = ### Fill in the missing code to read the file\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab16f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close() # Remember to close the file handle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5f6282",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "In this problem we have an NWB file with some data (the file we\n",
    "created in problem 1a) and we have acquired some new data the tiff\n",
    "file from problem 1b). We want to have all the data in a single file.\n",
    "We will use two approaches:\n",
    "1. Append the data to an existing nwb file on disk.\n",
    "2. Create a new nwb file in memory file and save it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504e7188",
   "metadata": {},
   "source": [
    "#### Create an appropriate interface\n",
    "First we create an appropriate interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6853b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_interface = TiffImagingInterface(file_path=movie_path, sampling_frequency=15.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14005d1b",
   "metadata": {},
   "source": [
    "#### Append data to an existing NWB file\n",
    "We copy the file we created in problem 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef16cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a copy of the file we created in  problem 1a\n",
    "import shutil\n",
    "prob2a_nwb_file = data_dir / \"problem_2a.nwb\"\n",
    "\n",
    "shutil.copyfile(prob1a_nwb_file, prob2a_nwb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98376cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_interface.run_conversion(prob2a_nwb_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae76331",
   "metadata": {},
   "source": [
    "#### Read in the generated file\n",
    "Use pynwb to read the generated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39fd125",
   "metadata": {},
   "outputs": [],
   "source": [
    "io =  ### Fill in missing code to read the NWB file\n",
    "nwbfile = io.read()\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9357d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close() # Close the file handle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8269cd",
   "metadata": {},
   "source": [
    "#### Create an NWB file in memory and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob2b_nwb_file = data_dir / \"problem_2b.nwb\"\n",
    "with NWBHDF5IO(prob1a_nwb_file, mode = 'r') as fin, NWBHDF5IO(prob2b_nwb_file, mode = 'w' ) as fout:\n",
    "    prob1a = fin.read() # Read nwb file from prob1a\n",
    "    tiff_interface.add_to_nwbfile(prob1a) # Add the photon information to prob1a, modifies in place\n",
    "    fout.export(fin, nwbfile=prob1a) # Export the new file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c2e4f5",
   "metadata": {},
   "source": [
    "##### Read in the generated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0fa21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to read in the file\n",
    "# 1. Open a filehandle\n",
    "# 2. Read the file in the variable my_file\n",
    "# 3. Write the variable as the last statement in the block to print notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cb2563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the filehandle you opened in the previous code block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d741332",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "In this problem we are looking at the scenario where we have an NWB file already. \n",
    "However, we would like to remove some information and save the result as an NWB file.\n",
    "We will start with the NWB file we created in problem 1b and remove the TwoPhotonSeries\n",
    "from acquisition. Note that you can only pop items from LabelledDict objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe7a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = NWBHDF5IO(prob1b_nwb_file, mode = 'r')\n",
    "prob1b = fin.read()\n",
    "prob1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05efc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(prob1b.acquisition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f4fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_photon = prob1b.acquisition.pop('TwoPhotonSeries')\n",
    "prob1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abaf250",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob3_nwb_file = data_dir / \"problem_3.nwb\"\n",
    "with NWBHDF5IO(prob3_nwb_file, mode = 'w' ) as fout:\n",
    "    fout.export(fin, nwbfile=prob1b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ddc9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb87a265",
   "metadata": {},
   "source": [
    "#### Read in the generated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2435fe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "with NWBHDF5IO(prob3_nwb_file, mode='r') as fin:\n",
    "    print(fin.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2acc425",
   "metadata": {},
   "source": [
    "## NWBwidgets\n",
    "A closer look at NWB widgets. We will look at a file from the DANDI archive. \n",
    "Run the code block below and then follow the instructions on the displayed widget.\n",
    " 1. Select DANDI using the radio button. \n",
    " 2. Select dandiset 4. \n",
    " 3. Select the nwb file for sub-P27CS\n",
    " 4. Press the button load file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed94778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nwbwidgets.panel import Panel\n",
    "Panel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00200b1",
   "metadata": {},
   "source": [
    "## Writing your own neuroconv interface\n",
    "We will take a look on how to write a simple neuroconv interface.\n",
    "Let's assume we have some TTL signals that we have saved in a matlab file.\n",
    "We would like to create an interface to convert such files to the nwb format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56094d45",
   "metadata": {},
   "source": [
    "### Create a mat file with the data we would like to convert.\n",
    "We will create a mat file with some random data. The file will also contain\n",
    "a label, and a frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb683d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import savemat\n",
    "data = np.outer(\n",
    "    np.random.choice(a=[0,1], p=[0.8, 0.2],replace=True, size=100), \n",
    "    np.ones(10)).reshape(-1)\n",
    "matdict = {'data': data, 'freq': 1000, 'label':'TTLStrobe'}\n",
    "savemat(data_dir/\"test.mat\", matdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7046dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "res = loadmat(data_dir/\"test.mat\")\n",
    "#res['freq'][0][0] # Obtains the frequency\n",
    "#res['label'][0]   # Obtains the label\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e91edb",
   "metadata": {},
   "source": [
    "### Looking at BaseDataInterface\n",
    "You can seee that is an Abstract data class and that we need to overwrite the `add_to_nwbfile`\n",
    "and `__init__` method of the BaseDataInterface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30041b97",
   "metadata": {},
   "source": [
    "```\n",
    "class BaseDataInterface(ABC):\n",
    "    \"\"\"Abstract class defining the structure of all DataInterfaces.\"\"\"\n",
    "\n",
    "    display_name: Union[str, None] = None\n",
    "    keywords: tuple[str] = tuple()\n",
    "    associated_suffixes: tuple[str] = tuple()\n",
    "    info: Union[str, None] = None\n",
    "\n",
    "    @classmethod\n",
    "    def get_source_schema(cls) -> dict:\n",
    "        \"\"\"Infer the JSON schema for the source_data from the method signature (annotation typing).\"\"\"\n",
    "        return get_json_schema_from_method_signature(cls, exclude=[\"source_data\"])\n",
    "\n",
    "    @classmethod\n",
    "    def validate_source(cls, source_data: dict, verbose: bool = False):\n",
    "        \"\"\"Validate source_data against Converter source_schema.\"\"\"\n",
    "        cls._validate_source_data(source_data=source_data, verbose=verbose)\n",
    "\n",
    "    def _validate_source_data(self, source_data: dict, verbose: bool = False):\n",
    "\n",
    "        encoder = _NWBSourceDataEncoder()\n",
    "        # The encoder produces a serialized object, so we deserialized it for comparison\n",
    "\n",
    "        serialized_source_data = encoder.encode(source_data)\n",
    "        decoded_source_data = json.loads(serialized_source_data)\n",
    "        source_schema = self.get_source_schema()\n",
    "        validate(instance=decoded_source_data, schema=source_schema)\n",
    "        if verbose:\n",
    "            print(\"Source data is valid!\")\n",
    "\n",
    "    @validate_call\n",
    "    def __init__(self, verbose: bool = False, **source_data):\n",
    "        self.verbose = verbose\n",
    "        self.source_data = source_data\n",
    "\n",
    "        self._validate_source_data(source_data=source_data, verbose=verbose)\n",
    "\n",
    "    def get_metadata_schema(self) -> dict:\n",
    "        \"\"\"Retrieve JSON schema for metadata.\"\"\"\n",
    "        metadata_schema = load_dict_from_file(Path(__file__).parent / \"schemas\" / \"base_metadata_schema.json\")\n",
    "        return metadata_schema\n",
    "\n",
    "    def get_metadata(self) -> DeepDict:\n",
    "        \"\"\"Child DataInterface classes should override this to match their metadata.\"\"\"\n",
    "        metadata = DeepDict()\n",
    "        metadata[\"NWBFile\"][\"session_description\"] = \"\"\n",
    "        metadata[\"NWBFile\"][\"identifier\"] = str(uuid.uuid4())\n",
    "\n",
    "        # Add NeuroConv watermark (overridden if going through the GUIDE)\n",
    "        neuroconv_version = importlib.metadata.version(\"neuroconv\")\n",
    "        metadata[\"NWBFile\"][\"source_script\"] = f\"Created using NeuroConv v{neuroconv_version}\"\n",
    "        metadata[\"NWBFile\"][\"source_script_file_name\"] = __file__  # Required for validation\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    def validate_metadata(self, metadata: dict, append_mode: bool = False) -> None:\n",
    "        \"\"\"Validate the metadata against the schema.\"\"\"\n",
    "        encoder = _NWBMetaDataEncoder()\n",
    "        # The encoder produces a serialized object, so we deserialized it for comparison\n",
    "\n",
    "        serialized_metadata = encoder.encode(metadata)\n",
    "        decoded_metadata = json.loads(serialized_metadata)\n",
    "        metdata_schema = self.get_metadata_schema()\n",
    "        if append_mode:\n",
    "            # Eliminate required from NWBFile\n",
    "            nwbfile_schema = metdata_schema[\"properties\"][\"NWBFile\"]\n",
    "            nwbfile_schema.pop(\"required\", None)\n",
    "\n",
    "        validate(instance=decoded_metadata, schema=metdata_schema)\n",
    "\n",
    "    def get_conversion_options_schema(self) -> dict:\n",
    "        \"\"\"Infer the JSON schema for the conversion options from the method signature (annotation typing).\"\"\"\n",
    "        return get_json_schema_from_method_signature(self.add_to_nwbfile, exclude=[\"nwbfile\", \"metadata\"])\n",
    "\n",
    "    def create_nwbfile(self, metadata: Optional[dict] = None, **conversion_options) -> NWBFile:\n",
    "        \"\"\"\n",
    "        Create and return an in-memory pynwb.NWBFile object with this interface's data added to it.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        metadata : dict, optional\n",
    "            Metadata dictionary with information used to create the NWBFile.\n",
    "        **conversion_options\n",
    "            Additional keyword arguments to pass to the `.add_to_nwbfile` method.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        nwbfile : pynwb.NWBFile\n",
    "            The in-memory object with this interface's data added to it.\n",
    "        \"\"\"\n",
    "        if metadata is None:\n",
    "            metadata = self.get_metadata()\n",
    "\n",
    "        nwbfile = make_nwbfile_from_metadata(metadata=metadata)\n",
    "        self.add_to_nwbfile(nwbfile=nwbfile, metadata=metadata, **conversion_options)\n",
    "\n",
    "        return nwbfile\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_to_nwbfile(self, nwbfile: NWBFile, **conversion_options) -> None:\n",
    "        \"\"\"\n",
    "        Define a protocol for mapping the data from this interface to NWB neurodata objects.\n",
    "\n",
    "        These neurodata objects should also be added to the in-memory pynwb.NWBFile object in this step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nwbfile : pynwb.NWBFile\n",
    "            The in-memory object to add the data to.\n",
    "        **conversion_options\n",
    "            Additional keyword arguments to pass to the `.add_to_nwbfile` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def run_conversion(\n",
    "        self,\n",
    "        nwbfile_path: FilePath,\n",
    "        nwbfile: Optional[NWBFile] = None,\n",
    "        metadata: Optional[dict] = None,\n",
    "        overwrite: bool = False,\n",
    "        backend: Optional[Literal[\"hdf5\", \"zarr\"]] = None,\n",
    "        backend_configuration: Optional[Union[HDF5BackendConfiguration, ZarrBackendConfiguration]] = None,\n",
    "        **conversion_options,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run the NWB conversion for the instantiated data interface.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nwbfile_path : FilePathType\n",
    "            Path for where the data will be written or appended.\n",
    "        nwbfile : NWBFile, optional\n",
    "            An in-memory NWBFile object to write to the location.\n",
    "        metadata : dict, optional\n",
    "            Metadata dictionary with information used to create the NWBFile when one does not exist or overwrite=True.\n",
    "        overwrite : bool, default: False\n",
    "            Whether to overwrite the NWBFile if one exists at the nwbfile_path.\n",
    "            The default is False (append mode).\n",
    "        backend : {\"hdf5\", \"zarr\"}, optional\n",
    "            The type of backend to use when writing the file.\n",
    "            If a `backend_configuration` is not specified, the default type will be \"hdf5\".\n",
    "            If a `backend_configuration` is specified, then the type will be auto-detected.\n",
    "        backend_configuration : HDF5BackendConfiguration or ZarrBackendConfiguration, optional\n",
    "            The configuration model to use when configuring the datasets for this backend.\n",
    "            To customize, call the `.get_default_backend_configuration(...)` method, modify the returned\n",
    "            BackendConfiguration object, and pass that instead.\n",
    "            Otherwise, all datasets will use default configuration settings.\n",
    "        \"\"\"\n",
    "\n",
    "        backend = _resolve_backend(backend, backend_configuration)\n",
    "        no_nwbfile_provided = nwbfile is None  # Otherwise, variable reference may mutate later on inside the context\n",
    "\n",
    "        if metadata is None:\n",
    "            metadata = self.get_metadata()\n",
    "\n",
    "        file_initially_exists = Path(nwbfile_path).exists() if nwbfile_path is not None else False\n",
    "        append_mode = file_initially_exists and not overwrite\n",
    "\n",
    "        self.validate_metadata(metadata=metadata, append_mode=append_mode)\n",
    "\n",
    "        with make_or_load_nwbfile(\n",
    "            nwbfile_path=nwbfile_path,\n",
    "            nwbfile=nwbfile,\n",
    "            metadata=metadata,\n",
    "            overwrite=overwrite,\n",
    "            backend=backend,\n",
    "            verbose=getattr(self, \"verbose\", False),\n",
    "        ) as nwbfile_out:\n",
    "            if no_nwbfile_provided:\n",
    "                self.add_to_nwbfile(nwbfile=nwbfile_out, metadata=metadata, **conversion_options)\n",
    "\n",
    "            if backend_configuration is None:\n",
    "                backend_configuration = self.get_default_backend_configuration(nwbfile=nwbfile_out, backend=backend)\n",
    "\n",
    "            configure_backend(nwbfile=nwbfile_out, backend_configuration=backend_configuration)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_backend_configuration(\n",
    "        nwbfile: NWBFile,\n",
    "        # TODO: when all H5DataIO prewraps are gone, introduce Zarr safely\n",
    "        # backend: Union[Literal[\"hdf5\", \"zarr\"]],\n",
    "        backend: Literal[\"hdf5\"] = \"hdf5\",\n",
    "    ) -> Union[HDF5BackendConfiguration, ZarrBackendConfiguration]:\n",
    "        \"\"\"\n",
    "        Fill and return a default backend configuration to serve as a starting point for further customization.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nwbfile : pynwb.NWBFile\n",
    "            The in-memory object with this interface's data already added to it.\n",
    "        backend : \"hdf5\", default: \"hdf5\"\n",
    "            The type of backend to use when creating the file.\n",
    "            Additional backend types will be added soon.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        backend_configuration : HDF5BackendConfiguration or ZarrBackendConfiguration\n",
    "            The default configuration for the specified backend type.\n",
    "        \"\"\"\n",
    "        return get_default_backend_configuration(nwbfile=nwbfile, backend=backend)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888e6106",
   "metadata": {},
   "source": [
    "### Imports \n",
    "We will use the following imports in constructing our class.\n",
    "The notebook format is not really appropriate for creating \n",
    "a class, this is something you would likely want to do\n",
    "as a python module. We are only using the notebook presentation\n",
    "to allow you to easily follow along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0815800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from neuroconv import BaseDataInterface\n",
    "from pydantic import FilePath\n",
    "from pydantic.validate_call_decorator import validate_call\n",
    "from scipy.io import loadmat\n",
    "from pynwb import NWBFile, TimeSeries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd8e2ac",
   "metadata": {},
   "source": [
    "### Extend base data interface class\n",
    "#### First attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950085ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatTTL(BaseDataInterface):\n",
    "    \"\"\" My class to convert matlab files to TLL\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1435fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MatTTL(True) # This is intended to fail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6347245",
   "metadata": {},
   "source": [
    "#### Second attempt\n",
    "We will need to add the method add_to_nwb_file to fix the previous error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331bc4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatTTL(BaseDataInterface):\n",
    "    \"\"\" My class to convert matlab files to TLL\"\"\"\n",
    "    \n",
    "\n",
    "    def add_to_nwbfile(self, nwbfile: NWBFile, metadata: Optional[dict], **conversion_options) -> None:\n",
    "        \"\"\"\n",
    "        Define a protocol for mapping the data from this interface to NWB neurodata objects.\n",
    "\n",
    "        These neurodata objects should also be added to the in-memory pynwb.NWBFile object in this step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nwbfile : pynwb.NWBFile\n",
    "            The in-memory object to add the data to.\n",
    "        **conversion_options\n",
    "            Additional keyword arguments to pass to the `.add_to_nwbfile` method.\n",
    "        \"\"\"\n",
    "        ts = TimeSeries(name=self.name, \n",
    "                        data=self.data, \n",
    "                        unit=\"V\", \n",
    "                        starting_time=self.starting_time, \n",
    "                        rate= self.rate)\n",
    "        nwbfile.add_acquisition(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9f37af",
   "metadata": {},
   "outputs": [],
   "source": [
    "MatTTL(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94619be",
   "metadata": {},
   "source": [
    "#### Third attempt\n",
    "Trying to use the interface above will generate run-time errors as the variables we have used are not already available. We will fix that by adding a constructor to our class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b7fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MatTTL(BaseDataInterface):\n",
    "    \"\"\" My class to convert matlab files to TTL \"\"\"\n",
    "    @validate_call\n",
    "    def __init__(self,\n",
    "                 file_path: FilePath,\n",
    "                 verbose: bool = True\n",
    "                 ):\n",
    "        super().__init__(verbose,file_path=file_path)\n",
    "        res = loadmat(file_path) # Read matlab file\n",
    "        self.starting_time = 0.0 # Assume that starting time is alway the start time of the session\n",
    "        self.name = res.get('label', ['TTLSignal'])[0]\n",
    "        self.rate = float(res.get('freq', [[1000]])[0][0])\n",
    "        self.data = res.get('data').reshape(-1)\n",
    "\n",
    "    def add_to_nwbfile(self, nwbfile: NWBFile, metadata: Optional[dict], **conversion_options) -> None:\n",
    "        ts = TimeSeries(name=self.name, \n",
    "                        data=self.data, \n",
    "                        unit=\"V\", \n",
    "                        starting_time=self.starting_time, \n",
    "                        rate= self.rate)\n",
    "        nwbfile.add_acquisition(ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7e0836",
   "metadata": {},
   "outputs": [],
   "source": [
    "MatTTL(file_path=data_dir/\"test.mat\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a545267",
   "metadata": {},
   "source": [
    "#### Using our interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f01b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "mat_file = data_dir/\"test.mat\"\n",
    "mat_nwb_file = data_dir /\"test.nwb\"\n",
    "mat_interface = MatTTL(file_path=mat_file, verbose=True)\n",
    "metadata = mat_interface.get_metadata()\n",
    "metadata['NWBFile']['session_start_time'] = datetime.datetime.now(tz=ZoneInfo(\"Europe/Paris\"))\n",
    "mat_interface.run_conversion(mat_nwb_file, metadata= metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c071e",
   "metadata": {},
   "source": [
    "#### Reading the generated file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58eeaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "fin = NWBHDF5IO(mat_nwb_file, mode = 'r')\n",
    "mat_nwb = fin.read()\n",
    "mat_nwb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ca5ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e4528",
   "metadata": {},
   "source": [
    "We can extend this by implementing other methods of the base class for example the get_metadata() method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8718b3bd",
   "metadata": {},
   "source": [
    "## Pynapple\n",
    "[Pynapple](https://pynapple.org/) is a python package aiming to make timeseries analysis easier.\n",
    "In the documentation you can find instructions for working with the following concepts.\n",
    "- Timeseries\n",
    "- Perievent\n",
    "- Correlation\n",
    "- Tuning curves\n",
    "- Spectrogram\n",
    "- Filtering\n",
    "\n",
    "In this workshop we will only take a cursory look at a small fraction of this functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f7c7f",
   "metadata": {},
   "source": [
    "#### Create some data series\n",
    "We start by creating some data series that we will \n",
    "use down the line to show some of the pynapple functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebe7218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pynwb import TimeSeries, NWBHDF5IO, NWBFile\n",
    "from pathlib import Path\n",
    "data_dir = Path(\"../data\")\n",
    "prob1b_nwb_file = data_dir / \"problem_1b.nwb\"\n",
    "\n",
    "t = 200.0 # We will make our longest time series to be 200 seconds\n",
    "# Sample rates in Hz\n",
    "f1 = 48000.0 \n",
    "f2 = 96000.0\n",
    "f3 = 10000.0\n",
    "# Time series starts\n",
    "s1 = 10.0\n",
    "s3 = s2 = 0.0\n",
    "# The time series objects\n",
    "y1 = TimeSeries(\"sin48k\",\n",
    "                data=np.sin(np.linspace(s1,t,num=int((t-s1)*f1))),\n",
    "                unit = \"V\",\n",
    "                starting_time=s1,\n",
    "                rate=f1)\n",
    "\n",
    "\n",
    "y2 = TimeSeries(\"sin96k\",\n",
    "                data=np.sin(np.linspace(s2,t,num=int((t-s2)*f2))),\n",
    "                unit = \"V\",\n",
    "                starting_time=s2,\n",
    "                rate=f2)\n",
    "              \n",
    "t3 = 50\n",
    "tpoints = np.linspace(0,t3,num=int(t3*f3))\n",
    "y3 = TimeSeries(\"composite\",\n",
    "                data= np.sin(2*np.pi*5*tpoints)+np.sin(2*np.pi*50*tpoints)+np.sin(2*np.pi*1000*tpoints),\n",
    "                rate=f3,\n",
    "                unit=\"V\",\n",
    "                starting_time=0.0)\n",
    "\n",
    "# Put all the time series  a an array\n",
    "my_timeseries = [y1, y2, y3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6df12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with NWBHDF5IO(prob1b_nwb_file, mode = 'a') as fio:\n",
    "    prob1b = fio.read() # Read nwb file from prob1a\n",
    "    for tseries in my_timeseries:\n",
    "        prob1b.add_acquisition(tseries)\n",
    "    fio.write(prob1b) # Export the appended file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba743ed4",
   "metadata": {},
   "source": [
    "### Read the NWB file with Pynapple\n",
    "At this stage before you run the next code cell restart the Kernel (this is to reduce the memory usage which is important in Binder where we have only 2GB of RAM available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83688ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynapple as nap\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "data_dir = Path(\"../data\")\n",
    "csv_file_path = data_dir / \"mydata.csv\" # Form file data path\n",
    "prob1b_nwb_file = data_dir / \"problem_1b.nwb\"\n",
    "nwb = nap.load_file(prob1b_nwb_file)\n",
    "print(nwb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da218b50",
   "metadata": {},
   "source": [
    "### Pynapple objects\n",
    "pynapple provides some objects to make our life easier various types of timeseries.\n",
    "#### Timeseries\n",
    "1. 1d timeseries (Tsd)\n",
    "2. 2d timeseries aka timeframes (TsdFrame)\n",
    "3. n-dimensional timeseries (TsdTensor)\n",
    "\n",
    "It also provides some auxiliary objects as time intervals (IntervalSet), timestamps (Ts) and a way to group different timeStamps/1d Timeseries (TsGroup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac53c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb[\"sin96k\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fb6c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb[\"TwoPhotonSeries\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316582aa",
   "metadata": {},
   "source": [
    "### Interval set\n",
    "We will create the intervals we care about. Note that pynapple has a bug and the\n",
    "data doesn't look right this is just a pretty printer error that hopefully\n",
    "will be fixed soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ae03b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intervals = nap.IntervalSet(start=pd.read_csv(csv_file_path).start_time, \n",
    "                            end=pd.read_csv(csv_file_path).end_time)\n",
    "intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals[16] # The data stored are correct but the representation above is wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978d5dd9",
   "metadata": {},
   "source": [
    "### Time support \n",
    "All time series have a time support property. You can also set the time support using the restrict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ea37a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb[\"sin96k\"].time_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9b260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb[\"TwoPhotonSeries\"].time_support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f01e23",
   "metadata": {},
   "source": [
    "### Restrict functionality\n",
    "We can restrict a timeseries objects to an interval or IntervalSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8774693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb[\"sin96k\"].restrict(intervals[16]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c5a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb[\"TwoPhotonSeries\"].restrict(intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14383405",
   "metadata": {},
   "source": [
    "#### Bin Count and Bin averaging\n",
    "We can count our data or average per bin. The result timestamps are the bin centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7266ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb[\"TwoPhotonSeries\"].count(ep=intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c75361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ycur = nwb[\"sin96k\"]\n",
    "ycur.count(1050,time_units='ms') # Count in bins of 1050ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9012d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ycur.bin_average(np.pi) # Which are about \\pm 2/pi as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29821dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ycur.bin_average(1,ep=intervals[0:5]) # Average over 1 second intervals over the intervals[0:5] support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d72297",
   "metadata": {},
   "source": [
    "### Threshold signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5669d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f27731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yp = ycur.restrict(nap.IntervalSet(start=[0],end=[8*np.pi]))\n",
    "plt.plot(yp)\n",
    "plt.plot(yp.threshold(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8de68f",
   "metadata": {},
   "source": [
    "### Use numpy function\n",
    "We can use numpy functionality as shown in the example\n",
    "below. Note that we are getting a TsdFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e787e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yp = np.mean(nwb[\"TwoPhotonSeries\"],1)\n",
    "print(type(yp))\n",
    "yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713b7fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yp[:,0:4]) # Convenience in plotting timeseries items (plot the first 4 columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b2dc27",
   "metadata": {},
   "source": [
    "### Autocorrelation functions\n",
    "Pynapple provides an autocorrelation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ee42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_group=nap.TsGroup({0:nwb['sin96k'], 1:nwb['sin48k']},time_support=nap.IntervalSet(start=0,end=15))\n",
    "print(ts_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad6e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_group=nap.TsGroup({0:nwb['sin96k'].bin_average(2e-3), 1:nwb['sin48k'].bin_average(1e-3)},time_support=nap.IntervalSet(start=4*np.pi,end=6*np.pi))\n",
    "print(ts_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04dcef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorrs = nap.compute_autocorrelogram(group=ts_group,binsize=np.pi/16, windowsize=4*np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9a3155",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(autocorrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf1f7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f21673",
   "metadata": {},
   "source": [
    "###  Signal processing functions\n",
    "It provides functions for power spectral calculation and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acef069",
   "metadata": {},
   "outputs": [],
   "source": [
    "psd = nap.compute_power_spectral_density(nwb['composite']) # Power spectral density\n",
    "plt.plot(psd)\n",
    "plt.xlim(0,1100)\n",
    "plt.xlabel(\"Frequency in Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccb98dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_50hz = nap.apply_bandpass_filter(nwb['composite'],(20,80), mode='sinc',transition_bandwidth=1e-4) # Filter application.\n",
    "plt.plot(signal_50hz)\n",
    "plt.plot(nap.Tsd(t=np.linspace(0,2/50,100),d=np.sin(2*np.pi*50*np.linspace(0,2/50,100))))\n",
    "plt.plot(nwb['composite'])\n",
    "plt.xlim(0,2/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb7319",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpass_filter = nap.get_filter_frequency_response((40,60), 10000, filter_type=\"bandpass\", \n",
    "                                           mode=\"sinc\",transition_bandwidth=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1dc6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bpass_filter)\n",
    "plt.xlim(0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aa4d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb.close() # Close nwb file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e48a8c",
   "metadata": {},
   "source": [
    "## Other ressources \n",
    "\n",
    "OpenScope Databook (Allen Institute) : https://alleninstitute.github.io/openscope_databook/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroconv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
