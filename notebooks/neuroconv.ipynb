{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f52409c6",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "If you were running this on your own machine you would have\n",
    "to install jupyter, neuroconv package and the data.\n",
    "\n",
    "However, if you are using either binder or github codespaces for the workshop these steps have already been done for you so you don't need to install anything.\n",
    "\n",
    "# NWB workshop\n",
    "## Loading and plotting NWB files\n",
    "\n",
    "In this section, you will load NWB datasets that you have already explored with the no-code tool \"GUIDE\".\n",
    "\n",
    "### Ex 1:\n",
    "Load a behavior dataset acquired with BASIL, look at the data structure and plot timeseries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70568e41-7a3a-44fb-9bcf-d485bc4f1d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "folder_path = \"/home/jovyan/data_nwb/BASIL/sub-390_ses-17.nwb\"\n",
    "io = NWBHDF5IO(folder_path, mode='r')\n",
    "nwbfile = io.read()\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaebd29-7226-46bc-9869-dae1cc224a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(nwbfile.acquisition['Lick'].data[100:20000])\n",
    "plt.plot(nwbfile.acquisition['TTLtrigsounds'].data[100:20000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7b3bb2",
   "metadata": {},
   "source": [
    "### Ex 2:\n",
    "Load an electrophysiology dataset (acquired with spikeglx) and preprocessing (performed with Kilosort), together with behavior/stimuli data (BASIL). Dataset courtesy : Pierre Platel.\n",
    "\n",
    "Look at the data structure and plot ephy raster plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b6617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder_path = \"/home/jovyan/data_nwb/ephy/spikeglx_kilosort_BASIL_NWBfile.nwb\"\n",
    "folder_path = \"C:/Users/cdussaux/OneDrive - Institut Pasteur Paris/Bureau/WorkshopNWB/4-OpenEphys_phy_basil/spikeglx_kilosort_BASIL_NWBfile.nwb\"\n",
    "io = NWBHDF5IO(folder_path, mode='r')\n",
    "nwbfile = io.read()\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9fd1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "units = nwbfile.units\n",
    "units_spike_times = units[\"spike_times\"]\n",
    "# bin size for counting spikes\n",
    "time_resolution = 0.01\n",
    "\n",
    "# start and end times (relative to the stimulus at 0 seconds) that we want to examine and align spikes to\n",
    "window_start_time = -0.1\n",
    "window_end_time = 2\n",
    "\n",
    "# time bins used\n",
    "n_bins = int((window_end_time - window_start_time) / time_resolution)\n",
    "bin_edges = np.linspace(window_start_time, window_end_time, n_bins, endpoint=True)\n",
    "\n",
    "# useful throughout analysis\n",
    "n_units = len(units_spike_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4987993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 1\n",
    "\n",
    "# 3D spike matrix to be populated with spike counts\n",
    "spike_matrix = np.zeros((n_units, len(bin_edges), n_trials))\n",
    "\n",
    "# populate 3D spike matrix for each unit for each stimulus trial by counting spikes into bins\n",
    "for unit_idx in range(n_units):\n",
    "    spike_times = units_spike_times[unit_idx]\n",
    "    \n",
    "    # get spike times that fall within the bin's time range relative to the stim time        \n",
    "    first_bin_time = bin_edges[0]\n",
    "    last_bin_time = bin_edges[-1]\n",
    "    first_spike_in_range, last_spike_in_range = np.searchsorted(spike_times, [first_bin_time, last_bin_time])\n",
    "    spike_times_in_range = spike_times[first_spike_in_range:last_spike_in_range]\n",
    "\n",
    "    # convert spike times into relative time bin indices\n",
    "    bin_indices = ((spike_times_in_range - (first_bin_time)) / time_resolution).astype(int)\n",
    "    \n",
    "    # mark that there is a spike at these bin times for this unit on this stim trial\n",
    "    for bin_idx in bin_indices:\n",
    "        spike_matrix[unit_idx, bin_idx, 0] += 1\n",
    "\n",
    "spike_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d9dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "trial = 0\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "ax.set_title(\"Unit Spikes\")\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax.set_ylabel(\"Unit #\")\n",
    "\n",
    "img = ax.imshow(spike_matrix[:,:,trial], extent=[window_start_time,window_end_time,0,n_units], aspect=0.001, vmin=0, vmax=1)\n",
    "cbar = fig.colorbar(img, shrink=0.5)\n",
    "cbar.set_label(\"# Spikes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d7228",
   "metadata": {},
   "source": [
    "### Ex 3:\n",
    "Load an optical imaging dataset and preprocessing (performed with Suite2P).\n",
    "\n",
    "Dataset from dandi.\n",
    "\n",
    "Look at the data structure and plot calcium imaging raster plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5862f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder_path = \"/home/jovyan/data_nwb/ophy/sub-171029-171110_ses-171110-a000_behavior+ophys.nwb\"\n",
    "folder_path = \"C:/Users/cdussaux/OneDrive - Institut Pasteur Paris/Bureau/WorkshopNWB/3-2P_behavior/dandi/sub-171029-171110_ses-171110-a000_behavior+ophys.nwb\"\n",
    "io = NWBHDF5IO(folder_path, mode='r')\n",
    "nwbfile = io.read()\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fb1346",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nwbfile.processing['ophys']\n",
    "Y = X.data_interfaces['fluorescence_all_cells']\n",
    "Z = Y.roi_response_series['raster dur v26_5_v37_6']\n",
    "\n",
    "timestamps = Z.timestamps\n",
    "data = Z.data\n",
    "\n",
    "print(f'timestamps shape: {timestamps.shape}')\n",
    "print(f'data shape: {data.shape}')\n",
    "n_cells = data.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b24414",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "ax.set_title(\"Raw fluorescence trace\")\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax.set_ylabel(\"Cells #\")\n",
    "\n",
    "img = ax.imshow(np.transpose(data[0:500,:]), aspect=0.25, vmin=0, vmax=1)\n",
    "cbar.set_label(\"# Spikes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba595a44-09b4-4886-9ebd-e2e42c812a40",
   "metadata": {},
   "source": [
    "## Converting to NWB with neuroconv\n",
    "\n",
    "We will deal with three problems that we think arise often.\n",
    "The first one is having a type of data that you would like\n",
    "to convert to NWB format and for which there is already a converter in [<img src=https://neuroconv.readthedocs.io/en/main/_images/neuroconv_logo.png width=50 height=50>](https://neuroconv.readthedocs.io/en/main/) The available converters can be found\n",
    "in the neuroconv [gallery](https://neuroconv.readthedocs.io/en/main/conversion_examples_gallery/index.html).\n",
    "The second one is already\n",
    "having an NWB file and some data, for which there is a neuroconv datainterface, that you would like to add to the NWB file. The third problem is having an NWB file from which you would like to extract some data and create a new file.\n",
    "\n",
    "\n",
    "### Problem 1a:\n",
    "We will start with the simplest case scenario.\n",
    "We have a single source of data. The data that we will use are from a csv file, \n",
    "and to convert them to NWB we will use the [`CsvTimeIntervalsInterface`](https://neuroconv.readthedocs.io/en/main/conversion_examples_gallery/text/csv.html) from neuroconv.\n",
    "The procedure will be the same for any type of data and the corresponding neuroconv interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1163d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requirement for the conversion\n",
    "from neuroconv.datainterfaces import CsvTimeIntervalsInterface\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a6074",
   "metadata": {},
   "source": [
    "#### Create CSV file if not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91454c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "data_dir = Path(\"../data\") # Data path\n",
    "data_dir.mkdir(exist_ok=True) # Create data dir\n",
    "csv_file_path = data_dir / \"mydata.csv\" # Form file data path\n",
    "random.seed(42) # set seed\n",
    "i = 0\n",
    "starts  = []\n",
    "ends  = []\n",
    "vals = []\n",
    "s0 = 0\n",
    "shift = 0.00001\n",
    "while (not ends) or ends[-1] < 120 :\n",
    "    starts.append(s0 + random.randrange(1,4))\n",
    "    end = starts[-1] + random.randrange(1,8)\n",
    "    vals.append(random.uniform(0,1))\n",
    "    s0 = end\n",
    "    ends.append(end)\n",
    "trial_times = pd.DataFrame({'start_time':starts, 'end_time':ends, 'value':vals })\n",
    "# Write csv file if it doesn't exist\n",
    "if not csv_file_path.exists():\n",
    "    trial_times.to_csv(csv_file_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014fc4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show first 5 content lines of csv file\n",
    "pd.read_csv(csv_file_path).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8440de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show first 5 content lines of csv file with bash\n",
    "### 1 line for the header + 5 lines for the contents\n",
    "!head -n6 ../data/mydata.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb71c1b",
   "metadata": {},
   "source": [
    "#### Create interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc721eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_interface = CsvTimeIntervalsInterface(file_path=csv_file_path, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dab7a04",
   "metadata": {},
   "source": [
    "#### Get metadata from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb97626",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = csv_interface.get_metadata()\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837e6d6",
   "metadata": {},
   "source": [
    "#### Attempt to create NWB file\n",
    "This is expected to Fail we just want to get an idea of the error messages and how to interpret them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8730f4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob1a_nwb_file = data_dir/\"problem_1a.nwb\"\n",
    "csv_interface.run_conversion(nwbfile_path = prob1a_nwb_file, metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03926ec9",
   "metadata": {},
   "source": [
    "#### Add missing metadata\n",
    "From the error message above we see that we are missing the session_start_time which is a required property.\n",
    "We will add that information in the metadata dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceac4983",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_start_time = datetime(2025,1,10,11,45,0, tzinfo=ZoneInfo(\"Europe/Paris\"))\n",
    "metadata[\"NWBFile\"][\"session_start_time\"] = session_start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e88b33",
   "metadata": {},
   "source": [
    "#### Create NWB file\n",
    "Now that we have have the required metadata we can try again to create the NWB file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a227e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_interface.run_conversion(nwbfile_path = prob1a_nwb_file, metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d317de",
   "metadata": {},
   "source": [
    "#### Look at the generated NWB file\n",
    "##### Use the pynwb library to read the file\n",
    "First we look at the file using the pynwb library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888c30d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "import pynwb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcf5874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideally we would use a with statement \n",
    "# as in the commented code\n",
    "#\n",
    "## with NWBHDF5IO(prob1a_nwb_file, mode = 'r') as io:\n",
    "##    nwbfile = io.read()\n",
    "##    nwbfile\n",
    "# we instead use the code below to be able to\n",
    "# see a nice version of the file.\n",
    "io = NWBHDF5IO(prob1a_nwb_file, mode = 'r')\n",
    "nwbfile = io.read()\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7407341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close() # Don't forget to close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f655e3b",
   "metadata": {},
   "source": [
    "##### Use NWBwidgets to read the file.\n",
    "Just a taste of how NWBwidgets work, we will take a closer look at NWBwidgets later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ca270f-a280-471d-831a-476805bc876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nwbwidgets import nwb2widget\n",
    "\n",
    "io = NWBHDF5IO(prob1a_nwb_file, mode = 'r')\n",
    "nwbfile = io.read()\n",
    "nwb2widget(nwbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0e1f9-6e7d-44c2-8110-4dfe883120e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee634be7",
   "metadata": {},
   "source": [
    "##### Use h5py to read the file\n",
    "Since our file is an HDF5 file we use any of the HDF5 libraries to read the file. Here we are using the h5py library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24357933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File(prob1a_nwb_file, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d26b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3dec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_intervals = f.get('intervals')\n",
    "my_print = lambda x,_: print(x)\n",
    "my_intervals.visititems(my_print)\n",
    "print(my_intervals.get('trials').get('value')[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a12d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ec48c3",
   "metadata": {},
   "source": [
    "###  Problem 1b\n",
    "Now we again want to generate a NWBFile from already available data.\n",
    "However, in addition to the csv file with some tiff images that \n",
    "should also be inside the generated NWB file.\n",
    "#### Download tiff file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017979b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download movie file if not already available\n",
    "if [[ ! -e \"../data/demoMovie.tif\" ]]; then\n",
    "   wget https://github.com/flatironinstitute/CaImAn/raw/refs/heads/main/example_movies/demoMovie.tif -O ../data/demoMovie.tif\n",
    "fi        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1d7b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroconv.datainterfaces import TiffImagingInterface\n",
    "from neuroconv import NWBConverter\n",
    "movie_path = data_dir / 'demoMovie.tif'\n",
    "prob1b_nwb_file = data_dir / 'problem_1b.nwb'\n",
    "\n",
    "\n",
    "class MyConverter(NWBConverter):\n",
    "    data_interface_classes = dict (\n",
    "        csvIntervals = CsvTimeIntervalsInterface,\n",
    "        movieRecording = TiffImagingInterface )\n",
    "\n",
    "sourceData = dict(\n",
    "      csvIntervals = dict(file_path=csv_file_path),\n",
    "      movieRecording = dict(file_path=movie_path, sampling_frequency=15.0))\n",
    "\n",
    "dual_converter = MyConverter(sourceData)\n",
    "\n",
    "metadata = dual_converter.get_metadata()\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e326706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[\"NWBFile\"][\"session_start_time\"] = session_start_time\n",
    "dual_converter.run_conversion(metadata=metadata, nwbfile_path=prob1b_nwb_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92153b91",
   "metadata": {},
   "source": [
    "#### Look at the generated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f455c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "io = NWBHDF5IO(prob1b_nwb_file, mode='r') \n",
    "nwbfile = ### Fill in the missing code to read the file\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab16f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close() # Remember to close the file handle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5f6282",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "In this problem we have an NWB file with some data (the file we\n",
    "created in problem 1a) and we have acquired some new data the tiff\n",
    "file from problem 1b). We want to have all the data in a single file.\n",
    "We will use two approaches:\n",
    "1. Append the data to an existing nwb file on disk.\n",
    "2. Create a new nwb file in memory file and save it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504e7188",
   "metadata": {},
   "source": [
    "#### Create an appropriate interface\n",
    "First we create an appropriate interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6853b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_interface = TiffImagingInterface(file_path=movie_path, sampling_frequency=15.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14005d1b",
   "metadata": {},
   "source": [
    "#### Append data to an existing NWB file\n",
    "We copy the file we created in problem 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef16cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a copy of the file we created in  problem 1a\n",
    "import shutil\n",
    "prob2a_nwb_file = data_dir / \"problem_2a.nwb\"\n",
    "\n",
    "shutil.copyfile(prob1a_nwb_file, prob2a_nwb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98376cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_interface.run_conversion(prob2a_nwb_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae76331",
   "metadata": {},
   "source": [
    "#### Read in the generated file\n",
    "Use pynwb to read the generated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39fd125",
   "metadata": {},
   "outputs": [],
   "source": [
    "io =  ### Fill in missing code to read the NWB file\n",
    "nwbfile = io.read()\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9357d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close() # Close the file handle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8269cd",
   "metadata": {},
   "source": [
    "#### Create an NWB file in memory and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob2b_nwb_file = data_dir / \"problem_2b.nwb\"\n",
    "with NWBHDF5IO(prob1a_nwb_file, mode = 'r') as fin, NWBHDF5IO(prob2b_nwb_file, mode = 'w' ) as fout:\n",
    "    prob1a = fin.read() # Read nwb file from prob1a\n",
    "    tiff_interface.add_to_nwbfile(prob1a) # Add the photon information to prob1a, modifies in place\n",
    "    fout.export(fin, nwbfile=prob1a) # Export the new file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c2e4f5",
   "metadata": {},
   "source": [
    "##### Read in the generated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0fa21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to read in the file\n",
    "# 1. Open a filehandle\n",
    "# 2. Read the file in the variable my_file\n",
    "# 3. Write the variable as the last statement in the block to print notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cb2563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the filehandle you opened in the previous code block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d741332",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "In this problem we are looking at the scenario where we have an NWB file already. \n",
    "However, we would like to remove some information and save the result as an NWB file.\n",
    "We will start with the NWB file we created in problem 1b and remove the TwoPhotonSeries\n",
    "from acquisition. Not you can pop items only from LabelledDict items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe7a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = NWBHDF5IO(prob1b_nwb_file, mode = 'r')\n",
    "prob1b = fin.read()\n",
    "prob1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05efc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(prob1b.acquisition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f4fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_photon = prob1b.acquisition.pop('TwoPhotonSeries')\n",
    "prob1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abaf250",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob3_nwb_file = data_dir / \"problem_3.nwb\"\n",
    "with NWBHDF5IO(prob3_nwb_file, mode = 'w' ) as fout:\n",
    "    fout.export(fin, nwbfile=prob1b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ddc9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb87a265",
   "metadata": {},
   "source": [
    "#### Read in the generated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2435fe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "with NWBHDF5IO(prob3_nwb_file, mode='r') as fin:\n",
    "    print(fin.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2acc425",
   "metadata": {},
   "source": [
    "## NWBwidgets\n",
    "A closer look at NWB widgets. We will look at a file from the DANDI archive. \n",
    "Run the code block below and then follow the instructions on the displayed widget.\n",
    " 1. Select DANDI using the radio button. \n",
    " 2. Select dandiset 4. \n",
    " 3. Select the nwb file for sub-P27CS\n",
    " 4. Press the button load file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed94778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nwbwidgets.panel import Panel\n",
    "Panel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00200b1",
   "metadata": {},
   "source": [
    "## Writing your own neuroconv interface\n",
    "We will take a look on how to write a simple neuroconv interface.\n",
    "Let's assume we have some TTL signals that we have saved in a matlab file.\n",
    "We would like to create an interface to convert such files to the nwb format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56094d45",
   "metadata": {},
   "source": [
    "### Create a mat file with the data we would like to convert.\n",
    "We will create a mat file with some random data. The file will also contain\n",
    "a label, and a frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb683d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import savemat\n",
    "data = np.outer(\n",
    "    np.random.choice(a=[0,1], p=[0.8, 0.2],replace=True, size=100), \n",
    "    np.ones(10)).reshape(-1)\n",
    "matdict = {'data': data, 'freq': 1000, 'label':'TTLStrobe'}\n",
    "savemat(data_dir/\"test.mat\", matdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7046dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "res = loadmat(data_dir/\"test.mat\")\n",
    "#res['freq'][0][0] # Obtains the frequency\n",
    "#res['label'][0]   # Obtains the label\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e91edb",
   "metadata": {},
   "source": [
    "### Looking at BaseDataInterface\n",
    "You can seee that is an Abstract data class and that we need to overwrite the `add_to_nwbfile`\n",
    "and `__init__` method of the BaseDataInterface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30041b97",
   "metadata": {},
   "source": [
    "```\n",
    "class BaseDataInterface(ABC):\n",
    "    \"\"\"Abstract class defining the structure of all DataInterfaces.\"\"\"\n",
    "\n",
    "    display_name: Union[str, None] = None\n",
    "    keywords: tuple[str] = tuple()\n",
    "    associated_suffixes: tuple[str] = tuple()\n",
    "    info: Union[str, None] = None\n",
    "\n",
    "    @classmethod\n",
    "    def get_source_schema(cls) -> dict:\n",
    "        \"\"\"Infer the JSON schema for the source_data from the method signature (annotation typing).\"\"\"\n",
    "        return get_json_schema_from_method_signature(cls, exclude=[\"source_data\"])\n",
    "\n",
    "    @classmethod\n",
    "    def validate_source(cls, source_data: dict, verbose: bool = False):\n",
    "        \"\"\"Validate source_data against Converter source_schema.\"\"\"\n",
    "        cls._validate_source_data(source_data=source_data, verbose=verbose)\n",
    "\n",
    "    def _validate_source_data(self, source_data: dict, verbose: bool = False):\n",
    "\n",
    "        encoder = _NWBSourceDataEncoder()\n",
    "        # The encoder produces a serialized object, so we deserialized it for comparison\n",
    "\n",
    "        serialized_source_data = encoder.encode(source_data)\n",
    "        decoded_source_data = json.loads(serialized_source_data)\n",
    "        source_schema = self.get_source_schema()\n",
    "        validate(instance=decoded_source_data, schema=source_schema)\n",
    "        if verbose:\n",
    "            print(\"Source data is valid!\")\n",
    "\n",
    "    @validate_call\n",
    "    def __init__(self, verbose: bool = False, **source_data):\n",
    "        self.verbose = verbose\n",
    "        self.source_data = source_data\n",
    "\n",
    "        self._validate_source_data(source_data=source_data, verbose=verbose)\n",
    "\n",
    "    def get_metadata_schema(self) -> dict:\n",
    "        \"\"\"Retrieve JSON schema for metadata.\"\"\"\n",
    "        metadata_schema = load_dict_from_file(Path(__file__).parent / \"schemas\" / \"base_metadata_schema.json\")\n",
    "        return metadata_schema\n",
    "\n",
    "    def get_metadata(self) -> DeepDict:\n",
    "        \"\"\"Child DataInterface classes should override this to match their metadata.\"\"\"\n",
    "        metadata = DeepDict()\n",
    "        metadata[\"NWBFile\"][\"session_description\"] = \"\"\n",
    "        metadata[\"NWBFile\"][\"identifier\"] = str(uuid.uuid4())\n",
    "\n",
    "        # Add NeuroConv watermark (overridden if going through the GUIDE)\n",
    "        neuroconv_version = importlib.metadata.version(\"neuroconv\")\n",
    "        metadata[\"NWBFile\"][\"source_script\"] = f\"Created using NeuroConv v{neuroconv_version}\"\n",
    "        metadata[\"NWBFile\"][\"source_script_file_name\"] = __file__  # Required for validation\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    def validate_metadata(self, metadata: dict, append_mode: bool = False) -> None:\n",
    "        \"\"\"Validate the metadata against the schema.\"\"\"\n",
    "        encoder = _NWBMetaDataEncoder()\n",
    "        # The encoder produces a serialized object, so we deserialized it for comparison\n",
    "\n",
    "        serialized_metadata = encoder.encode(metadata)\n",
    "        decoded_metadata = json.loads(serialized_metadata)\n",
    "        metdata_schema = self.get_metadata_schema()\n",
    "        if append_mode:\n",
    "            # Eliminate required from NWBFile\n",
    "            nwbfile_schema = metdata_schema[\"properties\"][\"NWBFile\"]\n",
    "            nwbfile_schema.pop(\"required\", None)\n",
    "\n",
    "        validate(instance=decoded_metadata, schema=metdata_schema)\n",
    "\n",
    "    def get_conversion_options_schema(self) -> dict:\n",
    "        \"\"\"Infer the JSON schema for the conversion options from the method signature (annotation typing).\"\"\"\n",
    "        return get_json_schema_from_method_signature(self.add_to_nwbfile, exclude=[\"nwbfile\", \"metadata\"])\n",
    "\n",
    "    def create_nwbfile(self, metadata: Optional[dict] = None, **conversion_options) -> NWBFile:\n",
    "        \"\"\"\n",
    "        Create and return an in-memory pynwb.NWBFile object with this interface's data added to it.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        metadata : dict, optional\n",
    "            Metadata dictionary with information used to create the NWBFile.\n",
    "        **conversion_options\n",
    "            Additional keyword arguments to pass to the `.add_to_nwbfile` method.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        nwbfile : pynwb.NWBFile\n",
    "            The in-memory object with this interface's data added to it.\n",
    "        \"\"\"\n",
    "        if metadata is None:\n",
    "            metadata = self.get_metadata()\n",
    "\n",
    "        nwbfile = make_nwbfile_from_metadata(metadata=metadata)\n",
    "        self.add_to_nwbfile(nwbfile=nwbfile, metadata=metadata, **conversion_options)\n",
    "\n",
    "        return nwbfile\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_to_nwbfile(self, nwbfile: NWBFile, **conversion_options) -> None:\n",
    "        \"\"\"\n",
    "        Define a protocol for mapping the data from this interface to NWB neurodata objects.\n",
    "\n",
    "        These neurodata objects should also be added to the in-memory pynwb.NWBFile object in this step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nwbfile : pynwb.NWBFile\n",
    "            The in-memory object to add the data to.\n",
    "        **conversion_options\n",
    "            Additional keyword arguments to pass to the `.add_to_nwbfile` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def run_conversion(\n",
    "        self,\n",
    "        nwbfile_path: FilePath,\n",
    "        nwbfile: Optional[NWBFile] = None,\n",
    "        metadata: Optional[dict] = None,\n",
    "        overwrite: bool = False,\n",
    "        backend: Optional[Literal[\"hdf5\", \"zarr\"]] = None,\n",
    "        backend_configuration: Optional[Union[HDF5BackendConfiguration, ZarrBackendConfiguration]] = None,\n",
    "        **conversion_options,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run the NWB conversion for the instantiated data interface.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nwbfile_path : FilePathType\n",
    "            Path for where the data will be written or appended.\n",
    "        nwbfile : NWBFile, optional\n",
    "            An in-memory NWBFile object to write to the location.\n",
    "        metadata : dict, optional\n",
    "            Metadata dictionary with information used to create the NWBFile when one does not exist or overwrite=True.\n",
    "        overwrite : bool, default: False\n",
    "            Whether to overwrite the NWBFile if one exists at the nwbfile_path.\n",
    "            The default is False (append mode).\n",
    "        backend : {\"hdf5\", \"zarr\"}, optional\n",
    "            The type of backend to use when writing the file.\n",
    "            If a `backend_configuration` is not specified, the default type will be \"hdf5\".\n",
    "            If a `backend_configuration` is specified, then the type will be auto-detected.\n",
    "        backend_configuration : HDF5BackendConfiguration or ZarrBackendConfiguration, optional\n",
    "            The configuration model to use when configuring the datasets for this backend.\n",
    "            To customize, call the `.get_default_backend_configuration(...)` method, modify the returned\n",
    "            BackendConfiguration object, and pass that instead.\n",
    "            Otherwise, all datasets will use default configuration settings.\n",
    "        \"\"\"\n",
    "\n",
    "        backend = _resolve_backend(backend, backend_configuration)\n",
    "        no_nwbfile_provided = nwbfile is None  # Otherwise, variable reference may mutate later on inside the context\n",
    "\n",
    "        if metadata is None:\n",
    "            metadata = self.get_metadata()\n",
    "\n",
    "        file_initially_exists = Path(nwbfile_path).exists() if nwbfile_path is not None else False\n",
    "        append_mode = file_initially_exists and not overwrite\n",
    "\n",
    "        self.validate_metadata(metadata=metadata, append_mode=append_mode)\n",
    "\n",
    "        with make_or_load_nwbfile(\n",
    "            nwbfile_path=nwbfile_path,\n",
    "            nwbfile=nwbfile,\n",
    "            metadata=metadata,\n",
    "            overwrite=overwrite,\n",
    "            backend=backend,\n",
    "            verbose=getattr(self, \"verbose\", False),\n",
    "        ) as nwbfile_out:\n",
    "            if no_nwbfile_provided:\n",
    "                self.add_to_nwbfile(nwbfile=nwbfile_out, metadata=metadata, **conversion_options)\n",
    "\n",
    "            if backend_configuration is None:\n",
    "                backend_configuration = self.get_default_backend_configuration(nwbfile=nwbfile_out, backend=backend)\n",
    "\n",
    "            configure_backend(nwbfile=nwbfile_out, backend_configuration=backend_configuration)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_backend_configuration(\n",
    "        nwbfile: NWBFile,\n",
    "        # TODO: when all H5DataIO prewraps are gone, introduce Zarr safely\n",
    "        # backend: Union[Literal[\"hdf5\", \"zarr\"]],\n",
    "        backend: Literal[\"hdf5\"] = \"hdf5\",\n",
    "    ) -> Union[HDF5BackendConfiguration, ZarrBackendConfiguration]:\n",
    "        \"\"\"\n",
    "        Fill and return a default backend configuration to serve as a starting point for further customization.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nwbfile : pynwb.NWBFile\n",
    "            The in-memory object with this interface's data already added to it.\n",
    "        backend : \"hdf5\", default: \"hdf5\"\n",
    "            The type of backend to use when creating the file.\n",
    "            Additional backend types will be added soon.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        backend_configuration : HDF5BackendConfiguration or ZarrBackendConfiguration\n",
    "            The default configuration for the specified backend type.\n",
    "        \"\"\"\n",
    "        return get_default_backend_configuration(nwbfile=nwbfile, backend=backend)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888e6106",
   "metadata": {},
   "source": [
    "### Imports \n",
    "We will use the following imports in constructing our class.\n",
    "The notebook format is not really appropriate for creating \n",
    "a class, this is something you would likely want to do\n",
    "as a python module. We are only using the notebook presentation\n",
    "to allow you to easily follow along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0815800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from neuroconv import BaseDataInterface\n",
    "from pydantic import FilePath\n",
    "from pydantic.validate_call_decorator import validate_call\n",
    "from scipy.io import loadmat\n",
    "from pynwb import NWBFile, TimeSeries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd8e2ac",
   "metadata": {},
   "source": [
    "### Extend base data interface class\n",
    "#### First attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950085ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatTTL(BaseDataInterface):\n",
    "    \"\"\" My class to convert matlab files to TLL\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1435fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MatTTL(True) # This is intended to fail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6347245",
   "metadata": {},
   "source": [
    "#### Second attempt\n",
    "We will need to add the method add_to_nwb_file to fix the previous error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331bc4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatTTL(BaseDataInterface):\n",
    "    \"\"\" My class to convert matlab files to TLL\"\"\"\n",
    "    \n",
    "\n",
    "    def add_to_nwbfile(self, nwbfile: NWBFile, metadata: Optional[dict], **conversion_options) -> None:\n",
    "        \"\"\"\n",
    "        Define a protocol for mapping the data from this interface to NWB neurodata objects.\n",
    "\n",
    "        These neurodata objects should also be added to the in-memory pynwb.NWBFile object in this step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nwbfile : pynwb.NWBFile\n",
    "            The in-memory object to add the data to.\n",
    "        **conversion_options\n",
    "            Additional keyword arguments to pass to the `.add_to_nwbfile` method.\n",
    "        \"\"\"\n",
    "        ts = TimeSeries(name=self.name, \n",
    "                        data=self.data, \n",
    "                        unit=\"V\", \n",
    "                        starting_time=self.starting_time, \n",
    "                        rate= self.rate)\n",
    "        nwbfile.add_acquisition(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9f37af",
   "metadata": {},
   "outputs": [],
   "source": [
    "MatTTL(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94619be",
   "metadata": {},
   "source": [
    "#### Third attempt\n",
    "Trying to use the interface above will generate run-time errors as the variables we have used are not already available. We will fix that by adding a constructor to our class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b7fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MatTTL(BaseDataInterface):\n",
    "    \"\"\" My class to convert matlab files to TTL \"\"\"\n",
    "    @validate_call\n",
    "    def __init__(self,\n",
    "                 file_path: FilePath,\n",
    "                 verbose: bool = True\n",
    "                 ):\n",
    "        super().__init__(verbose,file_path=file_path)\n",
    "        res = loadmat(file_path) # Read matlab file\n",
    "        self.starting_time = 0.0 # Assume that starting time is alway the start time of the session\n",
    "        self.name = res.get('label', ['TTLSignal'])[0]\n",
    "        self.rate = float(res.get('freq', [[1000]])[0][0])\n",
    "        self.data = res.get('data').reshape(-1)\n",
    "\n",
    "    def add_to_nwbfile(self, nwbfile: NWBFile, metadata: Optional[dict], **conversion_options) -> None:\n",
    "        ts = TimeSeries(name=self.name, \n",
    "                        data=self.data, \n",
    "                        unit=\"V\", \n",
    "                        starting_time=self.starting_time, \n",
    "                        rate= self.rate)\n",
    "        nwbfile.add_acquisition(ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7e0836",
   "metadata": {},
   "outputs": [],
   "source": [
    "MatTTL(file_path=data_dir/\"test.mat\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a545267",
   "metadata": {},
   "source": [
    "#### Using our interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f01b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "mat_file = data_dir/\"test.mat\"\n",
    "mat_nwb_file = data_dir /\"test.nwb\"\n",
    "mat_interface = MatTTL(file_path=mat_file, verbose=True)\n",
    "metadata = mat_interface.get_metadata()\n",
    "metadata['NWBFile']['session_start_time'] = datetime.datetime.now(tz=ZoneInfo(\"Europe/Paris\"))\n",
    "mat_interface.run_conversion(mat_nwb_file, metadata= metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c071e",
   "metadata": {},
   "source": [
    "#### Reading the generated file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58eeaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "fin = NWBHDF5IO(mat_nwb_file, mode = 'r')\n",
    "mat_nwb = fin.read()\n",
    "mat_nwb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ca5ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e4528",
   "metadata": {},
   "source": [
    "We can extend this by implementing other methods of the base class for example the get_metadata() method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8718b3bd",
   "metadata": {},
   "source": [
    "## Pynapple\n",
    "[Pynapple](https://pynapple.org/) is a python package aiming to make timeseries analysis easier.\n",
    "In the documentation you can find instructions for working with the following concepts.\n",
    "- Timeseries\n",
    "- Perievent\n",
    "- Correlation\n",
    "- Tuning curves\n",
    "- Spectrogram\n",
    "- Filtering\n",
    "\n",
    "In this workshop we will only take a cursory look at a small fraction of this functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f7c7f",
   "metadata": {},
   "source": [
    "#### Create some data series\n",
    "We start by creating some data series that we will \n",
    "use down the line to show some of the pynapple functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebe7218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pynwb import TimeSeries, NWBHDF5IO, NWBFile\n",
    "from pathlib import Path\n",
    "data_dir = Path(\"../data\")\n",
    "prob1b_nwb_file = data_dir / \"problem_1b.nwb\"\n",
    "\n",
    "t = 200.0 # We will make our longest time series to be 200 seconds\n",
    "# Sample rates in Hz\n",
    "f1 = 48000.0 \n",
    "f2 = 96000.0\n",
    "f3 = 44100.0\n",
    "# Time series starts\n",
    "s1 = 10.0\n",
    "s3=s2 = 0.0\n",
    "# The time series objects\n",
    "y1 = TimeSeries(\"sin48k\",\n",
    "                data=np.sin(np.linspace(s1,t,num=int((t-s1)*f1))),\n",
    "                unit = \"V\",\n",
    "                starting_time=s1,\n",
    "                rate=f1)\n",
    "\n",
    "\n",
    "y2= TimeSeries(\"sin96k\",\n",
    "                data=np.sin(np.linspace(s2,t,num=int((t-s2)*f2))),\n",
    "                unit = \"V\",\n",
    "                starting_time=s2,\n",
    "                rate=f2)\n",
    "              \n",
    "over_line = t + 20\n",
    "y3 = TimeSeries(\"sin44p1k\",\n",
    "                data=np.sin(np.linspace(s3,over_line-s3,num=int((over_line-s3)*f3))),\n",
    "                starting_time=s3,\n",
    "                unit = \"V\",\n",
    "                rate=f3)\n",
    "\n",
    "y4 = TimeSeries(\"noise\", \n",
    "                data=np.random.uniform(0,2,int(t*f3)),\n",
    "                rate=f3,\n",
    "                unit = \"V\",\n",
    "                starting_time=s2)\n",
    "tpoints = np.linspace(0,t,num=int(t*f1))\n",
    "y5 = TimeSeries(\"composite\",\n",
    "                data= np.sin(2*np.pi*5*tpoints)+np.sin(2*np.pi*50*tpoints)+np.sin(2*np.pi*1000*tpoints),\n",
    "                rate=f1,\n",
    "                unit=\"V\",\n",
    "                starting_time=0.0)\n",
    "# Put all the time series  a an array\n",
    "my_timeseries = [y1, y2, y3, y4, y5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6df12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with NWBHDF5IO(prob1b_nwb_file, mode = 'a') as fio:\n",
    "    prob1b = fio.read() # Read nwb file from prob1a\n",
    "    for tseries in my_timeseries:\n",
    "        prob1b.add_acquisition(tseries)\n",
    "    fio.write(prob1b) # Export the appended file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83688ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynapple as nap\n",
    "nwb = nap.load_file(prob1b_nwb_file)\n",
    "# or nwb = nap.NWBFile(\"../data/test.nwb)\n",
    "print(nwb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da218b50",
   "metadata": {},
   "source": [
    "### Pynapple objects\n",
    "pynapple provides some objects to make our life easier various types of timeseries.\n",
    "#### Timeseries\n",
    "1. 1d timeseries (Tsd)\n",
    "2. 2d timeseries aka timeframes (TsdFrame)\n",
    "3. n-dimensional timeseries (TsdTensor)\n",
    "\n",
    "It also provides some auxiliary objects as time intervals (IntervalSet), timestamps (Ts) and a way to group different timeStamps/1d Timeseries (TsGroup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac53c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb[\"sin96k\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fb6c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb[\"TwoPhotonSeries\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316582aa",
   "metadata": {},
   "source": [
    "### Interval set\n",
    "We will create the intervals we care about. Note that pynapple has a bug and the\n",
    "data doesn't look right this is just a pretty printer error that hopefully\n",
    "will be fixed soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ae03b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intervals = nap.IntervalSet(start=starts, end=ends)\n",
    "intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals[16] # The data stored are correct but the representation above is wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978d5dd9",
   "metadata": {},
   "source": [
    "### Time support \n",
    "All time series have a time support property. You can also set the time support using the restrict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ea37a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb[\"sin96k\"].time_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9b260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb[\"TwoPhotonSeries\"].time_support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f01e23",
   "metadata": {},
   "source": [
    "### Restrict functionality\n",
    "We can restrict a timeseries objects to an interval or IntervalSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8774693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb[\"sin96k\"].restrict(intervals[16]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c5a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb[\"TwoPhotonSeries\"].restrict(intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14383405",
   "metadata": {},
   "source": [
    "#### Bin Count and Bin averaging\n",
    "We can count our data or average per bin. The result timestamps are the bin centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7266ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb[\"TwoPhotonSeries\"].count(ep=intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c75361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ycur = nwb[\"sin96k\"]\n",
    "ycur.count(1050,time_units='ms') # Count in bins of 1050ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9012d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ycur.bin_average(np.pi) # Which are about \\pm 2/pi as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29821dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ycur.bin_average(1,ep=intervals[0:5]) # Average over 1 second intervals over the intervals[0:5] support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d72297",
   "metadata": {},
   "source": [
    "### Threshold signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5669d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f27731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yp = ycur.restrict(nap.IntervalSet(start=[0],end=[8*np.pi]))\n",
    "plt.plot(yp)\n",
    "plt.plot(yp.threshold(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8de68f",
   "metadata": {},
   "source": [
    "### Use numpy function\n",
    "We can use numpy functionality as shown in the example\n",
    "below. Note that we are getting a TsdFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e787e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yp = np.mean(nwb[\"TwoPhotonSeries\"],1)\n",
    "print(type(yp))\n",
    "yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713b7fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yp[:,0:4]) # Convenience in plotting timeseries items (plot the first 4 columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b2dc27",
   "metadata": {},
   "source": [
    "### Autocorrelation functions\n",
    "Pynapple provides an autocorrelation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ee42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_group=nap.TsGroup({0:nwb['sin96k'], 1:nwb['sin48k']},time_support=nap.IntervalSet(start=0,end=15))\n",
    "print(ts_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad6e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_group=nap.TsGroup({0:nwb['sin96k'].bin_average(100/f1), 1:nwb['sin48k'].bin_average(100/f1)},time_support=nap.IntervalSet(start=4*np.pi,end=6*np.pi))\n",
    "print(ts_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04dcef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorrs = nap.compute_autocorrelogram(group=ts_group,binsize=np.pi/16, windowsize=4*np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9a3155",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(autocorrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f21673",
   "metadata": {},
   "source": [
    "###  Signal processing functions\n",
    "It provides functions for power spectral calculation and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acef069",
   "metadata": {},
   "outputs": [],
   "source": [
    "psd = nap.compute_power_spectral_density(nwb['composite'],f1) # Power spectral density\n",
    "plt.plot(psd)\n",
    "plt.xlim(0,1100)\n",
    "plt.xlabel(\"Frequency in Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccb98dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_50hz = nap.apply_bandpass_filter(nwb['composite'],(20,80), mode='sinc',transition_bandwidth=1e-4) # Filter application.\n",
    "plt.plot(signal_50hz)\n",
    "plt.plot(tpoints,np.sin(2*np.pi*50*tpoints))\n",
    "plt.plot(nwb['composite'])\n",
    "plt.xlim(0,2/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb7319",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpass_filter = nap.get_filter_frequency_response((40,60), f1,filter_type=\"bandpass\", \n",
    "                                           mode=\"sinc\",transition_bandwidth=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1dc6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bpass_filter)\n",
    "plt.xlim(0,1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nwb_workshop",
   "language": "python",
   "name": "nwb_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
