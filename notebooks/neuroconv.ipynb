{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f52409c6",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "If you were running this on your own machine you would have to install jupyter, neuroconv package and the data.\n",
    "\n",
    "However, if you are using either binder or github codespaces for the workshop these steps have already been done for you so you don't need to install anything.\n",
    "\n",
    "For the GUIDE part of the workshop you will need to install GUIDE and download the data on your computer.\n",
    "\n",
    "1- Download, install and open GUIDE : https://nwb-guide.readthedocs.io/en/stable/\n",
    "\n",
    "2- Get datasets on your local machine :\n",
    "\n",
    "Download 2 NWB datasets on Pasteur Drive (Total 2.4GB)\n",
    "\n",
    "https://drive.pasteur.fr/s/ipLGkdP83Mwo975\n",
    "\n",
    "with the following password: WorkshopNWB@2025\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2aa79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pathlib import Path\n",
    "from pynwb import NWBHDF5IO\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a55cb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"../data\") # Data path\n",
    "data_dir.mkdir(exist_ok=True) # Create data dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba595a44-09b4-4886-9ebd-e2e42c812a40",
   "metadata": {},
   "source": [
    "## Converting to NWB with neuroconv\n",
    "\n",
    "We will deal with three problems that we think arise often.\n",
    "The first one is having a type of data that you would like\n",
    "to convert to NWB format and for which there is already a converter in [<img src=https://neuroconv.readthedocs.io/en/main/_images/neuroconv_logo.png width=50 height=50>](https://neuroconv.readthedocs.io/en/main/) The available converters can be found\n",
    "in the neuroconv [gallery](https://neuroconv.readthedocs.io/en/main/conversion_examples_gallery/index.html).\n",
    "The second one is already\n",
    "having an NWB file and some data, for which there is a neuroconv datainterface, that you would like to add to the NWB file. The third problem is having an NWB file from which you would like to extract some data and create a new file.\n",
    "\n",
    "\n",
    "### Problem 1a:\n",
    "We will start with the simplest case scenario.\n",
    "We have a single source of data. The data that we will use are from a csv file, \n",
    "and to convert them to NWB we will use the [`CsvTimeIntervalsInterface`](https://neuroconv.readthedocs.io/en/main/conversion_examples_gallery/text/csv.html) from neuroconv.\n",
    "The procedure will be the same for any type of data and the corresponding neuroconv interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1163d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requirement for the conversion\n",
    "from neuroconv.datainterfaces import CsvTimeIntervalsInterface\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a6074",
   "metadata": {},
   "source": [
    "#### Create CSV file if not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91454c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "data_dir = Path(\"../data\") # Data path\n",
    "data_dir.mkdir(exist_ok=True) # Create data dir\n",
    "csv_file_path = data_dir / \"mydata.csv\" # Form file data path\n",
    "random.seed(42) # set seed\n",
    "i = 0\n",
    "starts  = []\n",
    "ends  = []\n",
    "vals = []\n",
    "s0 = 0\n",
    "shift = 0.00001\n",
    "while (not ends) or ends[-1] < 120 :\n",
    "    starts.append(s0 + random.randrange(1,4))\n",
    "    end = starts[-1] + random.randrange(1,8)\n",
    "    vals.append(random.uniform(0,1))\n",
    "    s0 = end\n",
    "    ends.append(end)\n",
    "trial_times = pd.DataFrame({'start_time':starts, 'end_time':ends, 'value':vals })\n",
    "# Write csv file if it doesn't exist\n",
    "if not csv_file_path.exists():\n",
    "    trial_times.to_csv(csv_file_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014fc4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show first 5 content lines of csv file\n",
    "pd.read_csv(csv_file_path).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8440de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show first 5 content lines of csv file with bash\n",
    "### 1 line for the header + 5 lines for the contents\n",
    "!head -n6 ../data/mydata.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb71c1b",
   "metadata": {},
   "source": [
    "#### Create interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc721eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_interface = CsvTimeIntervalsInterface(file_path=csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dab7a04",
   "metadata": {},
   "source": [
    "#### Get metadata from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb97626",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = csv_interface.get_metadata()\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837e6d6",
   "metadata": {},
   "source": [
    "#### Attempt to create NWB file\n",
    "This is expected to Fail we just want to get an idea of the error messages and how to interpret them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8730f4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob1a_nwb_file = data_dir/\"problem_1a.nwb\"\n",
    "csv_interface.run_conversion(nwbfile_path = prob1a_nwb_file, metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03926ec9",
   "metadata": {},
   "source": [
    "#### Add missing metadata\n",
    "From the error message above we see that we are missing the session_start_time which is a required property.\n",
    "We will add that information in the metadata dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceac4983",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_start_time = datetime(2025,1,10,11,45,0, tzinfo=ZoneInfo(\"Europe/Paris\"))\n",
    "metadata[\"NWBFile\"][\"session_start_time\"] = session_start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e88b33",
   "metadata": {},
   "source": [
    "#### Create NWB file\n",
    "Now that we have have the required metadata we can try again to create the NWB file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a227e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_interface.run_conversion(nwbfile_path = prob1a_nwb_file, metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d317de",
   "metadata": {},
   "source": [
    "#### Look at the generated NWB file\n",
    "##### Use the pynwb library to read the file\n",
    "First we look at the file using the pynwb library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcf5874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideally we would use a with statement \n",
    "# as in the commented code\n",
    "#\n",
    "## with NWBHDF5IO(prob1a_nwb_file, mode = 'r') as io:\n",
    "##    nwbfile = io.read()\n",
    "##    nwbfile\n",
    "# we instead use the code below to be able to\n",
    "# see a nice version of the file.\n",
    "io = NWBHDF5IO(prob1a_nwb_file, mode = 'r')\n",
    "nwbfile = io.read()\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee634be7",
   "metadata": {},
   "source": [
    "##### Use h5py to read the file\n",
    "Since our file is an HDF5 file we use any of the HDF5 libraries to read the file. Here we are using the h5py library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24357933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File(prob1a_nwb_file, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d26b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3dec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_intervals = f.get('intervals')\n",
    "my_print = lambda x,_: print(x)\n",
    "my_intervals.visititems(my_print)\n",
    "print(my_intervals.get('trials').get('value')[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a12d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ec48c3",
   "metadata": {},
   "source": [
    "###  Problem 1b\n",
    "Now we again want to generate a NWBFile from already available data.\n",
    "However, in addition to the csv file with some tiff images that \n",
    "should also be inside the generated NWB file.\n",
    "#### Download tiff file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017979b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download movie file if not already available\n",
    "if [[ ! -e \"../data/demoMovie.tif\" ]]; then\n",
    "   wget --quiet https://github.com/flatironinstitute/CaImAn/raw/refs/heads/main/example_movies/demoMovie.tif -O ../data/demoMovie.tif\n",
    "fi        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1d7b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroconv.datainterfaces import TiffImagingInterface\n",
    "from neuroconv import NWBConverter\n",
    "movie_path = data_dir / 'demoMovie.tif'\n",
    "prob1b_nwb_file = data_dir / 'problem_1b.nwb'\n",
    "\n",
    "\n",
    "class MyConverter(NWBConverter):\n",
    "    data_interface_classes = dict (\n",
    "        csvIntervals = CsvTimeIntervalsInterface,\n",
    "        movieRecording = TiffImagingInterface )\n",
    "\n",
    "sourceData = dict(\n",
    "      csvIntervals = dict(file_path=csv_file_path),\n",
    "      movieRecording = dict(file_path=movie_path, sampling_frequency=15.0))\n",
    "\n",
    "dual_converter = MyConverter(sourceData)\n",
    "\n",
    "metadata = dual_converter.get_metadata()\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e326706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[\"NWBFile\"][\"session_start_time\"] = session_start_time\n",
    "dual_converter.run_conversion(metadata=metadata, nwbfile_path=prob1b_nwb_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92153b91",
   "metadata": {},
   "source": [
    "#### Look at the generated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f455c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "io = NWBHDF5IO(prob1b_nwb_file, mode = 'r')\n",
    "nwbfile = io.read()\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab16f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close() # Remember to close the file handle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5f6282",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "In this problem we have an NWB file with some data (the file we\n",
    "created in problem 1a) and we have acquired some new data the tiff\n",
    "file from problem 1b). We want to have all the data in a single file.\n",
    "We will use two approaches:\n",
    "1. Append the data to an existing nwb file on disk.\n",
    "2. Create a new nwb file in memory file and save it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504e7188",
   "metadata": {},
   "source": [
    "#### Create an appropriate interface\n",
    "First we create an appropriate interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6853b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_interface = TiffImagingInterface(file_path=movie_path, sampling_frequency=15.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14005d1b",
   "metadata": {},
   "source": [
    "#### Append data to an existing NWB file\n",
    "We copy the file we created in problem 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef16cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a copy of the file we created in  problem 1a\n",
    "import shutil\n",
    "prob2a_nwb_file = data_dir / \"problem_2a.nwb\"\n",
    "\n",
    "shutil.copyfile(prob1a_nwb_file, prob2a_nwb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aada23",
   "metadata": {},
   "outputs": [],
   "source": [
    "io = NWBHDF5IO(prob2a_nwb_file)\n",
    "nwbfile = io.read()\n",
    "nwbfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1083a1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98376cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_interface.run_conversion(nwbfile_path=prob2a_nwb_file, append_on_disk_nwbfile=True)\n",
    "#Bug with neuroconv fixed in the main repository version (and in the next release)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae76331",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Read in the generated file\n",
    "Use pynwb to read the generated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39fd125",
   "metadata": {},
   "outputs": [],
   "source": [
    "io =  NWBHDF5IO(prob2a_nwb_file,'r') \n",
    "nwbfile = io.read()\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9357d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close() # Close the file handle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8269cd",
   "metadata": {},
   "source": [
    "#### Create an NWB file in memory and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob2b_nwb_file = data_dir / \"problem_2b.nwb\"\n",
    "with NWBHDF5IO(prob1a_nwb_file, mode = 'r') as fin, NWBHDF5IO(prob2b_nwb_file, mode = 'w' ) as fout:\n",
    "    prob1a = fin.read() # Read nwb file from prob1a\n",
    "    tiff_interface.add_to_nwbfile(prob1a) # Add the photon information to prob1a, modifies in place\n",
    "    fout.export(fin, nwbfile=prob1a) # Export the new file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c2e4f5",
   "metadata": {},
   "source": [
    "##### Read in the generated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0fa21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to read in the file\n",
    "# 1. Open a filehandle\n",
    "# 2. Read the file in the variable my_file\n",
    "# 3. Write the variable as the last statement in the block to print notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cb2563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the filehandle you opened in the previous code block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d741332",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "In this problem we are looking at the scenario where we have an NWB file already. \n",
    "However, we would like to remove some information and save the result as an NWB file.\n",
    "We will start with the NWB file we created in problem 1b and remove the TwoPhotonSeries\n",
    "from acquisition. Note that you can only pop items from LabelledDict objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe7a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = NWBHDF5IO(prob1b_nwb_file, mode = 'r')\n",
    "prob1b = fin.read()\n",
    "prob1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05efc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(prob1b.acquisition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f4fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_photon = prob1b.acquisition.pop('TwoPhotonSeries')\n",
    "prob1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abaf250",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob3_nwb_file = data_dir / \"problem_3.nwb\"\n",
    "with NWBHDF5IO(prob3_nwb_file, mode = 'w' ) as fout:\n",
    "    fout.export(fin, nwbfile=prob1b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ddc9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb87a265",
   "metadata": {},
   "source": [
    "#### Read in the generated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2435fe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "with NWBHDF5IO(prob3_nwb_file, mode='r') as fin:\n",
    "    print(fin.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7db3fba-31f6-4f85-a57f-874973dc0eae",
   "metadata": {},
   "source": [
    "### Zarr backend (Problem 1a revisited)\n",
    "You can also create a zarr file (this is more of a directory structure) and allows for parallel reads and writes (ideal for s3 storage) but\n",
    "the generation of multiple files might not be optimal for standard file systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9fc049-e5ec-4c44-a254-4925018f9c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob1a_nwb_zarr_file = data_dir/\"problem_1a.zarr\"\n",
    "csv_interface.run_conversion(nwbfile_path = prob1a_nwb_zarr_file, metadata=metadata, backend='zarr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f24229-d269-4d7d-8db8-cc1641d92c65",
   "metadata": {},
   "source": [
    "#### Read the zarr file using hdmf_zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e36a6dc-ad8f-4e5b-9587-f601119555a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hdmf_zarr.nwb import NWBZarrIO\n",
    "io = NWBZarrIO(path=prob1a_nwb_zarr_file, mode=\"r\")\n",
    "nwb_zarr_file = io.read()\n",
    "nwb_zarr_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a43f15f-fcbf-4caf-af02-4399ca512129",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close() ## close the handle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5a8623-1620-4d8a-91c9-3ff64fae02d7",
   "metadata": {},
   "source": [
    "#### Look at the zarr file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48131f7d-cff9-4913-9131-7ecc51488d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from directory_tree import DisplayTree\n",
    "\n",
    "DisplayTree(prob1a_nwb_zarr_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed94778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nwbwidgets.panel import Panel\n",
    "Panel()\n",
    "#TODO: icephys_meta seems to be unmaintained causing problems with the update in hdmf.utils that changed\n",
    "# call_docval_func to docval_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00200b1",
   "metadata": {},
   "source": [
    "## Writing your own neuroconv interface\n",
    "We will take a look on how to write a simple neuroconv interface.\n",
    "Let's assume we have some TTL signals that we have saved in a matlab file.\n",
    "We would like to create an interface to convert such files to the nwb format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56094d45",
   "metadata": {},
   "source": [
    "### Create a mat file with the data we would like to convert.\n",
    "We will create a mat file with some random data. The file will also contain\n",
    "a label, and a frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb683d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import savemat\n",
    "data = np.outer(\n",
    "    np.random.choice(a=[0,1], p=[0.8, 0.2],replace=True, size=100), \n",
    "    np.ones(10)).reshape(-1)\n",
    "matdict = {'data': data, 'freq': 1000, 'label':'TTLStrobe'}\n",
    "savemat(data_dir/\"test.mat\", matdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7046dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "res = loadmat(data_dir/\"test.mat\")\n",
    "#res['freq'][0][0] # Obtains the frequency\n",
    "#res['label'][0]   # Obtains the label\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e91edb",
   "metadata": {},
   "source": [
    "### Looking at BaseDataInterface\n",
    "You can seee that is an Abstract data class and that we need to overwrite the `add_to_nwbfile`\n",
    "and `__init__` method of the BaseDataInterface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30041b97",
   "metadata": {},
   "source": [
    "```\n",
    "class BaseDataInterface(ABC):\n",
    "    \"\"\"Abstract class defining the structure of all DataInterfaces.\"\"\"\n",
    "\n",
    "    display_name: Union[str, None] = None\n",
    "    keywords: tuple[str] = tuple()\n",
    "    associated_suffixes: tuple[str] = tuple()\n",
    "    info: Union[str, None] = None\n",
    "\n",
    "    @classmethod\n",
    "    def get_source_schema(cls) -> dict:\n",
    "        \"\"\"Infer the JSON schema for the source_data from the method signature (annotation typing).\"\"\"\n",
    "        return get_json_schema_from_method_signature(cls, exclude=[\"source_data\"])\n",
    "\n",
    "    @classmethod\n",
    "    def validate_source(cls, source_data: dict, verbose: bool = False):\n",
    "        \"\"\"Validate source_data against Converter source_schema.\"\"\"\n",
    "        cls._validate_source_data(source_data=source_data, verbose=verbose)\n",
    "\n",
    "    def _validate_source_data(self, source_data: dict, verbose: bool = False):\n",
    "\n",
    "        encoder = _NWBSourceDataEncoder()\n",
    "        # The encoder produces a serialized object, so we deserialized it for comparison\n",
    "\n",
    "        serialized_source_data = encoder.encode(source_data)\n",
    "        decoded_source_data = json.loads(serialized_source_data)\n",
    "        source_schema = self.get_source_schema()\n",
    "        validate(instance=decoded_source_data, schema=source_schema)\n",
    "        if verbose:\n",
    "            print(\"Source data is valid!\")\n",
    "\n",
    "    @validate_call\n",
    "    def __init__(self, verbose: bool = False, **source_data):\n",
    "        self.verbose = verbose\n",
    "        self.source_data = source_data\n",
    "\n",
    "        self._validate_source_data(source_data=source_data, verbose=verbose)\n",
    "\n",
    "    def get_metadata_schema(self) -> dict:\n",
    "        \"\"\"Retrieve JSON schema for metadata.\"\"\"\n",
    "        metadata_schema = load_dict_from_file(Path(__file__).parent / \"schemas\" / \"base_metadata_schema.json\")\n",
    "        return metadata_schema\n",
    "\n",
    "    def get_metadata(self) -> DeepDict:\n",
    "        \"\"\"Child DataInterface classes should override this to match their metadata.\"\"\"\n",
    "        metadata = DeepDict()\n",
    "        metadata[\"NWBFile\"][\"session_description\"] = \"\"\n",
    "        metadata[\"NWBFile\"][\"identifier\"] = str(uuid.uuid4())\n",
    "\n",
    "        # Add NeuroConv watermark (overridden if going through the GUIDE)\n",
    "        neuroconv_version = importlib.metadata.version(\"neuroconv\")\n",
    "        metadata[\"NWBFile\"][\"source_script\"] = f\"Created using NeuroConv v{neuroconv_version}\"\n",
    "        metadata[\"NWBFile\"][\"source_script_file_name\"] = __file__  # Required for validation\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    def validate_metadata(self, metadata: dict, append_mode: bool = False) -> None:\n",
    "        \"\"\"Validate the metadata against the schema.\"\"\"\n",
    "        encoder = _NWBMetaDataEncoder()\n",
    "        # The encoder produces a serialized object, so we deserialized it for comparison\n",
    "\n",
    "        serialized_metadata = encoder.encode(metadata)\n",
    "        decoded_metadata = json.loads(serialized_metadata)\n",
    "        metdata_schema = self.get_metadata_schema()\n",
    "        if append_mode:\n",
    "            # Eliminate required from NWBFile\n",
    "            nwbfile_schema = metdata_schema[\"properties\"][\"NWBFile\"]\n",
    "            nwbfile_schema.pop(\"required\", None)\n",
    "\n",
    "        validate(instance=decoded_metadata, schema=metdata_schema)\n",
    "\n",
    "    def get_conversion_options_schema(self) -> dict:\n",
    "        \"\"\"Infer the JSON schema for the conversion options from the method signature (annotation typing).\"\"\"\n",
    "        return get_json_schema_from_method_signature(self.add_to_nwbfile, exclude=[\"nwbfile\", \"metadata\"])\n",
    "\n",
    "    def create_nwbfile(self, metadata: Optional[dict] = None, **conversion_options) -> NWBFile:\n",
    "        \"\"\"\n",
    "        Create and return an in-memory pynwb.NWBFile object with this interface's data added to it.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        metadata : dict, optional\n",
    "            Metadata dictionary with information used to create the NWBFile.\n",
    "        **conversion_options\n",
    "            Additional keyword arguments to pass to the `.add_to_nwbfile` method.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        nwbfile : pynwb.NWBFile\n",
    "            The in-memory object with this interface's data added to it.\n",
    "        \"\"\"\n",
    "        if metadata is None:\n",
    "            metadata = self.get_metadata()\n",
    "\n",
    "        nwbfile = make_nwbfile_from_metadata(metadata=metadata)\n",
    "        self.add_to_nwbfile(nwbfile=nwbfile, metadata=metadata, **conversion_options)\n",
    "\n",
    "        return nwbfile\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_to_nwbfile(self, nwbfile: NWBFile, **conversion_options) -> None:\n",
    "        \"\"\"\n",
    "        Define a protocol for mapping the data from this interface to NWB neurodata objects.\n",
    "\n",
    "        These neurodata objects should also be added to the in-memory pynwb.NWBFile object in this step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nwbfile : pynwb.NWBFile\n",
    "            The in-memory object to add the data to.\n",
    "        **conversion_options\n",
    "            Additional keyword arguments to pass to the `.add_to_nwbfile` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def run_conversion(\n",
    "        self,\n",
    "        nwbfile_path: FilePath,\n",
    "        nwbfile: Optional[NWBFile] = None,\n",
    "        metadata: Optional[dict] = None,\n",
    "        overwrite: bool = False,\n",
    "        backend: Optional[Literal[\"hdf5\", \"zarr\"]] = None,\n",
    "        backend_configuration: Optional[Union[HDF5BackendConfiguration, ZarrBackendConfiguration]] = None,\n",
    "        **conversion_options,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run the NWB conversion for the instantiated data interface.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nwbfile_path : FilePathType\n",
    "            Path for where the data will be written or appended.\n",
    "        nwbfile : NWBFile, optional\n",
    "            An in-memory NWBFile object to write to the location.\n",
    "        metadata : dict, optional\n",
    "            Metadata dictionary with information used to create the NWBFile when one does not exist or overwrite=True.\n",
    "        overwrite : bool, default: False\n",
    "            Whether to overwrite the NWBFile if one exists at the nwbfile_path.\n",
    "            The default is False (append mode).\n",
    "        backend : {\"hdf5\", \"zarr\"}, optional\n",
    "            The type of backend to use when writing the file.\n",
    "            If a `backend_configuration` is not specified, the default type will be \"hdf5\".\n",
    "            If a `backend_configuration` is specified, then the type will be auto-detected.\n",
    "        backend_configuration : HDF5BackendConfiguration or ZarrBackendConfiguration, optional\n",
    "            The configuration model to use when configuring the datasets for this backend.\n",
    "            To customize, call the `.get_default_backend_configuration(...)` method, modify the returned\n",
    "            BackendConfiguration object, and pass that instead.\n",
    "            Otherwise, all datasets will use default configuration settings.\n",
    "        \"\"\"\n",
    "\n",
    "        backend = _resolve_backend(backend, backend_configuration)\n",
    "        no_nwbfile_provided = nwbfile is None  # Otherwise, variable reference may mutate later on inside the context\n",
    "\n",
    "        if metadata is None:\n",
    "            metadata = self.get_metadata()\n",
    "\n",
    "        file_initially_exists = Path(nwbfile_path).exists() if nwbfile_path is not None else False\n",
    "        append_mode = file_initially_exists and not overwrite\n",
    "\n",
    "        self.validate_metadata(metadata=metadata, append_mode=append_mode)\n",
    "\n",
    "        with make_or_load_nwbfile(\n",
    "            nwbfile_path=nwbfile_path,\n",
    "            nwbfile=nwbfile,\n",
    "            metadata=metadata,\n",
    "            overwrite=overwrite,\n",
    "            backend=backend,\n",
    "            verbose=getattr(self, \"verbose\", False),\n",
    "        ) as nwbfile_out:\n",
    "            if no_nwbfile_provided:\n",
    "                self.add_to_nwbfile(nwbfile=nwbfile_out, metadata=metadata, **conversion_options)\n",
    "\n",
    "            if backend_configuration is None:\n",
    "                backend_configuration = self.get_default_backend_configuration(nwbfile=nwbfile_out, backend=backend)\n",
    "\n",
    "            configure_backend(nwbfile=nwbfile_out, backend_configuration=backend_configuration)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_backend_configuration(\n",
    "        nwbfile: NWBFile,\n",
    "        # TODO: when all H5DataIO prewraps are gone, introduce Zarr safely\n",
    "        # backend: Union[Literal[\"hdf5\", \"zarr\"]],\n",
    "        backend: Literal[\"hdf5\"] = \"hdf5\",\n",
    "    ) -> Union[HDF5BackendConfiguration, ZarrBackendConfiguration]:\n",
    "        \"\"\"\n",
    "        Fill and return a default backend configuration to serve as a starting point for further customization.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nwbfile : pynwb.NWBFile\n",
    "            The in-memory object with this interface's data already added to it.\n",
    "        backend : \"hdf5\", default: \"hdf5\"\n",
    "            The type of backend to use when creating the file.\n",
    "            Additional backend types will be added soon.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        backend_configuration : HDF5BackendConfiguration or ZarrBackendConfiguration\n",
    "            The default configuration for the specified backend type.\n",
    "        \"\"\"\n",
    "        return get_default_backend_configuration(nwbfile=nwbfile, backend=backend)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888e6106",
   "metadata": {},
   "source": [
    "### Imports \n",
    "We will use the following imports in constructing our class.\n",
    "The notebook format is not really appropriate for creating \n",
    "a class, this is something you would likely want to do\n",
    "as a python module. We are only using the notebook presentation\n",
    "to allow you to easily follow along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0815800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from neuroconv import BaseDataInterface\n",
    "from pydantic import FilePath\n",
    "from pydantic.validate_call_decorator import validate_call\n",
    "from scipy.io import loadmat\n",
    "from pynwb import NWBFile, TimeSeries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd8e2ac",
   "metadata": {},
   "source": [
    "### Extend base data interface class\n",
    "#### First attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950085ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatTTL(BaseDataInterface):\n",
    "    \"\"\" My class to convert matlab files to TLL\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1435fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MatTTL(True) # This is intended to fail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6347245",
   "metadata": {},
   "source": [
    "#### Second attempt\n",
    "We will need to add the method add_to_nwb_file to fix the previous error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331bc4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatTTL(BaseDataInterface):\n",
    "    \"\"\" My class to convert matlab files to TLL\"\"\"\n",
    "    \n",
    "\n",
    "    def add_to_nwbfile(self, nwbfile: NWBFile, metadata: Optional[dict], **conversion_options) -> None:\n",
    "        \"\"\"\n",
    "        Define a protocol for mapping the data from this interface to NWB neurodata objects.\n",
    "\n",
    "        These neurodata objects should also be added to the in-memory pynwb.NWBFile object in this step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nwbfile : pynwb.NWBFile\n",
    "            The in-memory object to add the data to.\n",
    "        **conversion_options\n",
    "            Additional keyword arguments to pass to the `.add_to_nwbfile` method.\n",
    "        \"\"\"\n",
    "        ts = TimeSeries(name=self.name, \n",
    "                        data=self.data, \n",
    "                        unit=\"V\", \n",
    "                        starting_time=self.starting_time, \n",
    "                        rate= self.rate)\n",
    "        nwbfile.add_acquisition(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9f37af",
   "metadata": {},
   "outputs": [],
   "source": [
    "MatTTL(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94619be",
   "metadata": {},
   "source": [
    "#### Third attempt\n",
    "Trying to use the interface above will generate run-time errors as the variables we have used are not already available. We will fix that by adding a constructor to our class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b7fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MatTTL(BaseDataInterface):\n",
    "    \"\"\" My class to convert matlab files to TTL \"\"\"\n",
    "    @validate_call\n",
    "    def __init__(self,\n",
    "                 file_path: FilePath,\n",
    "                 verbose: bool = True\n",
    "                 ):\n",
    "        super().__init__(verbose,file_path=file_path)\n",
    "        res = loadmat(file_path) # Read matlab file\n",
    "        self.starting_time = 0.0 # Assume that starting time is alway the start time of the session\n",
    "        self.name = res.get('label', ['TTLSignal'])[0]\n",
    "        self.rate = float(res.get('freq', [[1000]])[0][0])\n",
    "        self.data = res.get('data').reshape(-1)\n",
    "\n",
    "    def add_to_nwbfile(self, nwbfile: NWBFile, metadata: Optional[dict], **conversion_options) -> None:\n",
    "        ts = TimeSeries(name=self.name, \n",
    "                        data=self.data, \n",
    "                        unit=\"V\", \n",
    "                        starting_time=self.starting_time, \n",
    "                        rate= self.rate)\n",
    "        nwbfile.add_acquisition(ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7e0836",
   "metadata": {},
   "outputs": [],
   "source": [
    "MatTTL(file_path=data_dir/\"test.mat\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a545267",
   "metadata": {},
   "source": [
    "#### Using our interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f01b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "mat_file = data_dir/\"test.mat\"\n",
    "mat_nwb_file = data_dir /\"test.nwb\"\n",
    "mat_interface = MatTTL(file_path=mat_file, verbose=True)\n",
    "metadata = mat_interface.get_metadata()\n",
    "metadata['NWBFile']['session_start_time'] = datetime.datetime.now(tz=ZoneInfo(\"Europe/Paris\"))\n",
    "mat_interface.run_conversion(mat_nwb_file, metadata= metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c071e",
   "metadata": {},
   "source": [
    "#### Reading the generated file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58eeaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "fin = NWBHDF5IO(mat_nwb_file, mode = 'r')\n",
    "mat_nwb = fin.read()\n",
    "mat_nwb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ca5ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e4528",
   "metadata": {},
   "source": [
    "We can extend this by implementing other methods of the base class for example the get_metadata() method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8718b3bd",
   "metadata": {},
   "source": [
    "## Pynapple\n",
    "[Pynapple](https://pynapple.org/) is a python package aiming to make timeseries analysis easier.\n",
    "In the documentation you can find instructions for working with the following concepts.\n",
    "- Timeseries\n",
    "- Perievent\n",
    "- Correlation\n",
    "- Tuning curves\n",
    "- Spectrogram\n",
    "- Filtering\n",
    "\n",
    "In this workshop we will only take a cursory look at a small fraction of this functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f7c7f",
   "metadata": {},
   "source": [
    "#### Create some data series\n",
    "We start by creating some data series that we will \n",
    "use down the line to show some of the pynapple functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebe7218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pynwb import TimeSeries, NWBHDF5IO, NWBFile\n",
    "from pathlib import Path\n",
    "data_dir = Path(\"../data\")\n",
    "prob1b_nwb_file = data_dir / \"problem_1b.nwb\"\n",
    "\n",
    "t = 200.0 # We will make our longest time series to be 200 seconds\n",
    "# Sample rates in Hz\n",
    "f1 = 48000.0 \n",
    "f2 = 96000.0\n",
    "f3 = 10000.0\n",
    "# Time series starts\n",
    "s1 = 10.0\n",
    "s3 = s2 = 0.0\n",
    "# The time series objects\n",
    "y1 = TimeSeries(name=\"sin48k\",\n",
    "                data=np.sin(np.linspace(s1,t,num=int((t-s1)*f1))),\n",
    "                unit = \"V\",\n",
    "                starting_time=s1,\n",
    "                rate=f1)\n",
    "\n",
    "\n",
    "y2 = TimeSeries(name=\"sin96k\",\n",
    "                data=np.sin(np.linspace(s2,t,num=int((t-s2)*f2))),\n",
    "                unit = \"V\",\n",
    "                starting_time=s2,\n",
    "                rate=f2)\n",
    "              \n",
    "t3 = 50\n",
    "tpoints = np.linspace(0,t3,num=int(t3*f3))\n",
    "y3 = TimeSeries(name=\"composite\",\n",
    "                data= np.sin(2*np.pi*5*tpoints)+np.sin(2*np.pi*50*tpoints)+np.sin(2*np.pi*1000*tpoints),\n",
    "                rate=f3,\n",
    "                unit=\"V\",\n",
    "                starting_time=0.0)\n",
    "\n",
    "# Put all the time series  a an array\n",
    "my_timeseries = [y1, y2, y3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6df12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with NWBHDF5IO(prob1b_nwb_file, mode = 'a') as fio:\n",
    "    prob1b = fio.read() # Read nwb file from prob1a\n",
    "    for tseries in my_timeseries:\n",
    "        prob1b.add_acquisition(tseries)\n",
    "    fio.write(prob1b) # Export the appended file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba743ed4",
   "metadata": {},
   "source": [
    "### Read the NWB file with Pynapple\n",
    "At this stage before you run the next code cell restart the Kernel (this is to reduce the memory usage which is important in Binder where we have only 2GB of RAM available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83688ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynapple as nap\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "data_dir = Path(\"../data\")\n",
    "csv_file_path = data_dir / \"mydata.csv\" # Form file data path\n",
    "prob1b_nwb_file = data_dir / \"problem_1b.nwb\"\n",
    "nwb = nap.load_file(prob1b_nwb_file)\n",
    "print(nwb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da218b50",
   "metadata": {},
   "source": [
    "### Pynapple objects\n",
    "pynapple provides some objects to make our life easier various types of timeseries.\n",
    "#### Timeseries\n",
    "1. 1d timeseries (Tsd)\n",
    "2. 2d timeseries aka timeframes (TsdFrame)\n",
    "3. n-dimensional timeseries (TsdTensor)\n",
    "\n",
    "It also provides some auxiliary objects as time intervals (IntervalSet), timestamps (Ts) and a way to group different timeStamps/1d Timeseries (TsGroup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac53c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb[\"sin96k\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fb6c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb[\"TwoPhotonSeries\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316582aa",
   "metadata": {},
   "source": [
    "### Interval set\n",
    "We will create the intervals we care about. Note that pynapple has a bug and the\n",
    "data doesn't look right this is just a pretty printer error that hopefully\n",
    "will be fixed soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ae03b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intervals = nap.IntervalSet(start=pd.read_csv(csv_file_path).start_time, \n",
    "                            end=pd.read_csv(csv_file_path).end_time)\n",
    "intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals[16] # The data stored are correct but the representation above is wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978d5dd9",
   "metadata": {},
   "source": [
    "### Time support \n",
    "All time series have a time support property. You can also set the time support using the restrict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ea37a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb[\"sin96k\"].time_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9b260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb[\"TwoPhotonSeries\"].time_support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f01e23",
   "metadata": {},
   "source": [
    "### Restrict functionality\n",
    "We can restrict a timeseries objects to an interval or IntervalSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8774693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb[\"sin96k\"].restrict(intervals[16]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c5a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb[\"TwoPhotonSeries\"].restrict(intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14383405",
   "metadata": {},
   "source": [
    "#### Bin Count and Bin averaging\n",
    "We can count our data or average per bin. The result timestamps are the bin centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7266ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb[\"TwoPhotonSeries\"].count(ep=intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c75361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ycur = nwb[\"sin96k\"]\n",
    "ycur.count(1050,time_units='ms') # Count in bins of 1050ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9012d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ycur.bin_average(np.pi) # Which are about \\pm 2/pi as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29821dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ycur.bin_average(1,ep=intervals[0:5]) # Average over 1 second intervals over the intervals[0:5] support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d72297",
   "metadata": {},
   "source": [
    "### Threshold signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5669d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f27731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yp = ycur.restrict(nap.IntervalSet(start=[0],end=[8*np.pi]))\n",
    "plt.plot(yp)\n",
    "plt.plot(yp.threshold(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8de68f",
   "metadata": {},
   "source": [
    "### Use numpy function\n",
    "We can use numpy functionality as shown in the example\n",
    "below. Note that we are getting a TsdFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e787e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yp = np.mean(nwb[\"TwoPhotonSeries\"],1)\n",
    "print(type(yp))\n",
    "yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713b7fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yp[:,0:4]) # Convenience in plotting timeseries items (plot the first 4 columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf1f7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f21673",
   "metadata": {},
   "source": [
    "###  Signal processing functions\n",
    "It provides functions for power spectral calculation and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acef069",
   "metadata": {},
   "outputs": [],
   "source": [
    "psd = nap.compute_power_spectral_density(nwb['composite']) # Power spectral density\n",
    "plt.plot(psd)\n",
    "plt.xlim(0,1100)\n",
    "plt.xlabel(\"Frequency in Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccb98dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_50hz = nap.apply_bandpass_filter(nwb['composite'],(20,80), mode='sinc',transition_bandwidth=1e-4) # Filter application.\n",
    "plt.plot(signal_50hz)\n",
    "plt.plot(nap.Tsd(t=np.linspace(0,2/50,100),d=np.sin(2*np.pi*50*np.linspace(0,2/50,100))))\n",
    "plt.plot(nwb['composite'])\n",
    "plt.xlim(0,2/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb7319",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpass_filter = nap.get_filter_frequency_response((40,60), 10000, filter_type=\"bandpass\", \n",
    "                                           mode=\"sinc\",transition_bandwidth=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1dc6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bpass_filter)\n",
    "plt.xlim(0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aa4d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb.close() # Close nwb file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d195f4d2-445b-4d8a-b73f-60a79e8fe340",
   "metadata": {},
   "source": [
    "# Working with real data\n",
    "\n",
    "At this stage please restart your kernel, otherwise you might run out of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef01e31-22a2-4dd3-bd7e-bb87a9ec1ec8",
   "metadata": {},
   "source": [
    "## Download data for notebook use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a37fec7-3a0e-47ad-b27b-d179cb5f3f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ../data\n",
    "# Download BASIL dataset on the binder workspace\n",
    "for file in {sub-3502_ses-13.nwb,sub-716_ses-27_res.nwb,WhichSound.bin}; do\n",
    "    if [[ ! -e \"../data/${file}\" ]]; then\n",
    "        wget --quiet --user \"ipLGkdP83Mwo975\" --password \"WorkshopNWB@2025\" \"https://drive.pasteur.fr/public.php/webdav/${file}\" -O \"../data/${file}\" &\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c01a49-7575-434c-87eb-63f23a2f61a5",
   "metadata": {},
   "source": [
    "## PART 1 : Reading & Exploring a NWB file \n",
    "\n",
    "### Dataset 1 : Behavioral data (Go/NoGo auditory task)\n",
    "\n",
    "In this section, you will load NWB datasets with the no-code tool \"GUIDE\".\n",
    "\n",
    "Here, we explore a dataset acquired with BASIL app which is used to train mice to operant conditionning auditory discrimination task. One sound (Go) is associated to a reward and another sound (Nogo) is associated to no reward / punishment.\n",
    "\n",
    "1- In the GUIDE app, click Explore, load and find in your local repository the file sub-3502_ses-13.nwb\n",
    "\n",
    "3502 is the subject (mouse) ID.\n",
    "\n",
    "13 is the session (acquisition) ID.\n",
    "\n",
    "2- Expand the Neurodata Tab\n",
    "\n",
    "3- Check the Lick and click the blue button on top. Zoom to 20s of experiments. \n",
    "\n",
    "4- Close and now check these 3 items : Lick, Reward, SoundCopy. Click the blue button on top. Go a bit after the beginning.  \n",
    "Explanation on how to interpret these data are documented in the \"description\" part.\n",
    "\n",
    "5- Close and now check these 3 items : stimulus template id 0, 9 and 15. These are pure tones stimulus, of duration 500ms, at different frequencies. \n",
    "\n",
    "6- Close. Now, in Matlab or Python, with few line of codes, you can open and visualize the data in a notebook as follow : \n",
    "\n",
    "### Look at the data structure and visualize behavior data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b0b8de-f0e6-44ec-b37e-36091543a18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pynwb import NWBHDF5IO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6368821-8743-4661-8ab9-f95bba35b75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../data/sub-3502_ses-13.nwb\"\n",
    "io = NWBHDF5IO(folder_path, mode='r')\n",
    "nwbfile = io.read()\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6db73aa-6443-426e-b532-ab659e7e8f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Reward data \n",
    "rewarddata = nwbfile.acquisition['Reward'].data\n",
    "Bin = 1./nwbfile.acquisition['Reward'].rate\n",
    "tps_trial = np.arange(1, len(rewarddata) + 1) * Bin\n",
    "\n",
    "# Load trial type data \n",
    "trialid = nwbfile.acquisition['TrialType'].data\n",
    "\n",
    "# Load Lick data data\n",
    "Lickdata = nwbfile.acquisition['Lick'].data\n",
    "Binlick = 1./nwbfile.acquisition['Lick'].rate\n",
    "tps_data = np.arange(1, len(Lickdata) + 1) * Binlick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df118537-2e79-4106-b7ad-2dcafbbe505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize timeseries\n",
    "plt.plot(tps_data[:], Lickdata[:])\n",
    "plt.plot(tps_trial[:], trialid[:])\n",
    "plt.plot(tps_trial[:], rewarddata[:], \"r-\")\n",
    "\n",
    "plt.ylim([0, 10])\n",
    "plt.xlim([300, 320])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc50900-f0f1-454b-ad06-9c3e09580fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load whichsound.bin - in an ulterior version, it will be included in the NWB. \n",
    "def read_whichsound_bin(fpath):\n",
    "    file_path = fpath + 'WhichSound.bin'\n",
    "    dataid = np.fromfile(file_path, dtype='float64')\n",
    "    return dataid\n",
    "\n",
    "fpath = '../data/'\n",
    "dataid = read_whichsound_bin(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94f5d16-8d58-457d-bd69-134bf71007a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sequence_simple_auto(arr):\n",
    "    \"\"\"\n",
    "    Function to find the sound id list, removing repetition, from the whichsound vector\n",
    "    \"\"\"\n",
    "    if len(arr) == 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Find the minimum sequence of sound id in the vector\n",
    "    lengths = []\n",
    "    for key, group in groupby(arr):\n",
    "        length = len(list(group))\n",
    "        lengths.append(length)\n",
    "    \n",
    "    # Keep the smaller sequence (of at least one element)\n",
    "    repetition_length = min(lengths) if lengths else 1\n",
    "        \n",
    "    result = []\n",
    "    for key, group in groupby(arr):\n",
    "        group_list = list(group)\n",
    "        num_occurrences = len(group_list) // repetition_length\n",
    "        result.extend([key] * num_occurrences)\n",
    "    \n",
    "    return np.array(result)\n",
    "\n",
    "# Get the unique sequence of sound if in the whichsound timeseries\n",
    "AllTrials = process_sequence_simple_auto(dataid)\n",
    "print(AllTrials[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94b12be-b419-4957-8d08-3334641f854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete NaN values and get the total number of sound in the sequence\n",
    "valid_trials = AllTrials[~np.isnan(AllTrials)]\n",
    "AllTrials = AllTrials[~np.isnan(AllTrials)]\n",
    "\n",
    "if len(valid_trials) > 0:\n",
    "    SoundNum = int(np.nanmax(np.unique(valid_trials)))\n",
    "else:\n",
    "    SoundNum = 0\n",
    "\n",
    "print(f\"Sounds in the sequence : {SoundNum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b20d0e-74d7-4cab-a5d9-f04dd6e3e02c",
   "metadata": {},
   "source": [
    "Plotting of a psychometric curve.\n",
    "\n",
    "This plot models the relationship between a given feature of a physical stimulus, e.g. frequency, intensity etc., and forced-choice responses of a human or animal test subject.\n",
    "\n",
    "Here : \n",
    "\n",
    "\n",
    "Sounds 0 to 7 corresponds to NOGO\n",
    "\n",
    "Sounds 8 to 16 corresponds to GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea2d10a-33a9-4b80-ad5c-58c0b7871f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build the Correct vector which allow to quantify animal performances during the session\n",
    "\n",
    "trialid = np.array(trialid)\n",
    "trial_start_id = np.where(trialid == 9)[0]\n",
    "sound_start_id = np.where(trialid == 99)[0]\n",
    "\n",
    "# Create Correct_out with the appropriate length\n",
    "Correct_out = np.zeros(len(sound_start_id), dtype=int)\n",
    "\n",
    "for k in range(len(sound_start_id)):\n",
    "    if k == len(sound_start_id) - 1:\n",
    "        respwin = trialid[trial_start_id[k]+1:]\n",
    "        rewardid = rewarddata[trial_start_id[k]+1:]\n",
    "    else:\n",
    "        # Regular case\n",
    "        respwin = trialid[trial_start_id[k]+1 : trial_start_id[k+1]-1]\n",
    "        rewardid = rewarddata[trial_start_id[k]+1 : trial_start_id[k+1]-1]\n",
    "    \n",
    "    # Delete 99 values (beginning of the sound)\n",
    "    respwin = respwin[respwin != 99]\n",
    "    \n",
    "    # Get unique values\n",
    "    outcometrial = np.unique(respwin)\n",
    "    \n",
    "    try:\n",
    "        rewardout = rewardid[rewardid > 0]\n",
    "    except:\n",
    "        rewardout = np.array([0])\n",
    "    \n",
    "    # Delete value 4 (manual reward)\n",
    "    rewardout = rewardout[rewardout != 4]\n",
    "    \n",
    "    if len(rewardout) == 0:\n",
    "        rewardout = np.array([0])\n",
    "    \n",
    "    # 1 if correct trial, 0 otherwise\n",
    "    if (3 in rewardout) or (1 in rewardout):\n",
    "        Correct_out[k] = 1\n",
    "    elif (0 in rewardout) and (2 in outcometrial):\n",
    "        Correct_out[k] = 1\n",
    "    else:\n",
    "        Correct_out[k] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6f2c3d-fd0a-4fbe-8aa9-b8e55850cbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Correct = np.array(Correct_out)\n",
    "Perf = np.nanmean(Correct)\n",
    "\n",
    "# Compute correct trials as compare to total of trial and plot the psychometric curve\n",
    "if SoundNum > 0:\n",
    "    PropCorrect = np.full(SoundNum, np.nan)\n",
    "    for sd in range(1, SoundNum + 1):\n",
    "        indices = (AllTrials == sd)\n",
    "        if np.any(indices):\n",
    "            PropCorrect[sd-1] = np.nanmean(Correct[indices])\n",
    "    \n",
    "    # Invert the first part (FA)\n",
    "    PropCorrect[0:SoundNum//2] = 1 - PropCorrect[0:SoundNum//2]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, SoundNum + 1), PropCorrect, '.-', linewidth=3, markersize=20)\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlabel('Sound Identity')\n",
    "    plt.ylabel('High proba')\n",
    "    plt.axvline(x=SoundNum/2 + 0.5, color='k', linestyle='-')\n",
    "    plt.title(f'Performance: {Perf:.3f}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data to analysze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573cf1ca-50a6-4e60-9e93-323326bf4db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47f15af-31b2-4284-8b70-101cd378396e",
   "metadata": {},
   "source": [
    "### Dataset 2 : Electrophysiological recordings + Sound stimuli\n",
    "\n",
    "Here, we explore a dataset including : \n",
    "\n",
    "- Preprocessing / Spike sorting of Neuropixel recordings acquired with OpenEphys. Preprocessing is done with Kilosort and data curation is performed with Phy tool.  \n",
    "\n",
    "- Behavior/stimuli data acquired with BASIL.\n",
    "\n",
    "- EVENT TTL from BASIL to ephys for offline data synchronization.\n",
    "\n",
    "1- In the GUIDE app, click Explore, load and find in your local repository the file sub-716_ses-27_res.nwb\n",
    "\n",
    "716 is the subject (mouse) ID.\n",
    "\n",
    "27 is the session (acquisition) ID.\n",
    "\n",
    "Dataset courtesy : Pierre Platel, DSAPM, Hearing Institute.\n",
    "\n",
    "2- Expand the Neurodata Tab\n",
    "\n",
    "3- Expand the acquisition Tab. \n",
    "\n",
    "4- Check the Lick and click the blue button on top. Zoom to 20s of experiments. \n",
    "\n",
    "5- Close and now check 2 of these items : Lick, TTLTrignpx. Click the blue button on top. Zoom on 50s. \n",
    "\n",
    "6- Close. With few line of codes, you can open and visualize the data in a notebook as follow : \n",
    "\n",
    "### Look at the data structure and visualize behavior data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4023dcd0-d97d-48c8-9281-0a0ce8b0f166",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../data/sub-716_ses-27_res.nwb\"\n",
    "io = NWBHDF5IO(folder_path, mode='r')\n",
    "nwbfile = io.read()\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4824fd-ba81-48ac-9220-f8ff1503bd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize timeseries\n",
    "plt.plot(nwbfile.acquisition['Lick'].data[100:20000])\n",
    "plt.plot(nwbfile.acquisition['TTLtrignpx'].data[100:20000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf4f692-ba25-4873-9bf2-2d26ed707f16",
   "metadata": {},
   "source": [
    "7- Go back to GUIDE app, expand the unit Tab and have a look at the units description. \n",
    "\n",
    "8- Click the \"Raster\" button in the \"units\" checkbox.\n",
    "\n",
    "Select > 100 units with the \"+\" \n",
    "\n",
    "Slide on 10s time windows.\n",
    "\n",
    "9- Close and go to Python usage. With few line of codes, you can open and visualize the data in a notebook as follow :\n",
    "\n",
    "\n",
    "### Look at the data structure and plot spike raster plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfb18ff-8ab0-4f5f-9eaa-4d7c53f1fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "units = nwbfile.units\n",
    "units_spike_times = units[\"spike_times\"]\n",
    "# bin size for counting spikes\n",
    "time_resolution = 0.01\n",
    "\n",
    "# start and end times (relative to the stimulus at 0 seconds) that we want to examine and align spikes to\n",
    "window_start_time = -0.1\n",
    "window_end_time = 2\n",
    "\n",
    "# time bins used\n",
    "n_bins = int((window_end_time - window_start_time) / time_resolution)\n",
    "bin_edges = np.linspace(window_start_time, window_end_time, n_bins, endpoint=True)\n",
    "\n",
    "# useful throughout analysis\n",
    "n_units = len(units_spike_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23a81c7-e1f9-4779-adf4-e2f3e3f50942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D spike matrix to be populated with spike counts\n",
    "spike_matrix = np.zeros((n_units, len(bin_edges)))\n",
    "\n",
    "# populate 3D spike matrix for each unit for each stimulus trial by counting spikes into bins\n",
    "for unit_idx in range(n_units):\n",
    "    spike_times = units_spike_times[unit_idx]\n",
    "    \n",
    "    # get spike times that fall within the bin's time range relative to the stim time        \n",
    "    first_bin_time = bin_edges[0]\n",
    "    last_bin_time = bin_edges[-1]\n",
    "    first_spike_in_range, last_spike_in_range = np.searchsorted(spike_times, [first_bin_time, last_bin_time])\n",
    "    spike_times_in_range = spike_times[first_spike_in_range:last_spike_in_range]\n",
    "\n",
    "    # convert spike times into relative time bin indices\n",
    "    bin_indices = ((spike_times_in_range - (first_bin_time)) / time_resolution).astype(int)\n",
    "    \n",
    "    # mark that there is a spike at these bin times for this unit on this stim trial\n",
    "    for bin_idx in bin_indices:\n",
    "        spike_matrix[unit_idx, bin_idx] += 1\n",
    "\n",
    "spike_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5dd1bf-81e0-4d63-bb98-7eec397a6ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "ax.set_title(\"Unit Spikes\")\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax.set_ylabel(\"Unit #\")\n",
    "\n",
    "img = ax.imshow(spike_matrix[:,:], extent=[window_start_time,window_end_time,0,n_units], aspect=0.001, vmin=0, vmax=1)\n",
    "cbar = fig.colorbar(img, shrink=0.5)\n",
    "cbar.set_label(\"# Spikes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38117d1c-03c1-4cfb-b092-88e4c7fd92ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316edcd2-04be-41e8-945e-b77f3a72c9d8",
   "metadata": {},
   "source": [
    "## PART 2 : Building a multimodal NWB file using neuroconv\n",
    "\n",
    "The files used for this section are not provided because of size constraints.\n",
    "This parts is going to be a demo.\n",
    "\n",
    "In this section, you will create the previous NWB dataset 2 using : \n",
    "\n",
    "1- NWB BASIL file\n",
    "\n",
    "2- RAW ephys file (just for the example but too heavy)\n",
    "\n",
    "3- RAW Kilosort + Phy file\n",
    "\n",
    "4- EVENT TTL from BASIL to ephys for synchronisation\n",
    "\n",
    "5- Behavior video file as external link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87c4975-cefa-43ea-b67a-f6d83f8cb01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pathlib import Path\n",
    "from pynwb import NWBHDF5IO\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pynwb import TimeSeries\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3aaed1-ce8b-4879-a6ee-6f824f31a91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide saving and data paths\n",
    "saving_data_dir = Path(\"C:/Users/cdussaux/Documents/Python/NWB_conversion/data\")\n",
    "\n",
    "# Select BASIL NWB file \n",
    "basil_NWB_dir = Path(\"C:/Users/cdussaux/Documents/Python/NWB_workshop/NWBworkshop/data\")\n",
    "basil_NWB_filename = \"sub-716_ses-27.nwb\" \n",
    "basil_NWBdata = basil_NWB_dir / basil_NWB_filename\n",
    "\n",
    "# Select open ephys RAW data file (folder and stream) \n",
    "#folder_path_ephys = \"U:/Bathellierlab_gaia/Data/Electrophysiology/Pierre/RAW_DATA/716_20250915_DelGNG_AC&M2_3&3/Record Node 112\"\n",
    "#stream_name = 'Record Node 112#Neuropix-PXI-116.ProbeB-AP'\n",
    "\n",
    "# Select RAW preprocessed data (spike sorting)\n",
    "folder_path_phy = \"C:/Users/cdussaux/Documents/Python/NWB_workshop/NWBworkshop/data/716_20250915_DelGNG_M2_3\"\n",
    "\n",
    "# Select event TTL sent by BASIL to open ephys \n",
    "stimTTLtmp = \"C:/Users/cdussaux/Documents/Python/NWB_workshop/NWBworkshop/data/stimTTL/timestamps.npy\" #\"/experiment1/recording1/events/Neuropix-PXI-116.ProbeA-AP/TTL/timestamps.npy\"\n",
    "stimTTL = stimTTLtmp #folder_path_ephys + stimTTLtmp\n",
    "\n",
    "# Select NPX timestamp\n",
    "timestamptmp = \"C:/Users/cdussaux/Documents/Python/NWB_workshop/NWBworkshop/data/timestamp/timestamps.npy\" #\"/experiment1/recording1/continuous/Neuropix-PXI-116.ProbeA-AP/timestamps.npy\"\n",
    "timestamp = timestamptmp # folder_path_ephys + timestamptmp\n",
    "\n",
    "# Select Video file\n",
    "raw_video_path = \"C:/Users/cdussaux/Documents/Python/NWB_workshop/NWBworkshop/data/video/\"  #\"U:/Bathellierlab_gaia/BASIL/BASIL_FAIR/BASILapp/Results/Pierre/M716/20250915/143731_Data/\"\n",
    "video_file_path = raw_video_path + \"MouseVideo1.avi\"\n",
    "timing_files = raw_video_path + \"VideoTiming1.mat\"\n",
    "\n",
    "# Add csv log file to NWB\n",
    "csv_path = \"C:/Users/cdussaux/Documents/Python/NWB_workshop/NWBworkshop/data/\"\n",
    "csv_file_path = csv_path + \"Trial_log.csv\"\n",
    "csv_file_path_copy = csv_path + \"Trial_log_copy.csv\"\n",
    "\n",
    "# name of the new file is duplicated in the saving_data_dir.\n",
    "saving_data_filename, ext = os.path.splitext(basil_NWB_filename)\n",
    "saving_data_filename = saving_data_filename + \"_res\" + ext\n",
    "NWBdata_concatenate = saving_data_dir / saving_data_filename\n",
    "NWBdata_concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e465fc09-372c-4322-9b96-ba2827bf4261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of BASIL nwb file\n",
    "shutil.copyfile(basil_NWBdata, NWBdata_concatenate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf0952a-7f4d-4e45-b3a0-0179d6f4a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "io1 =  NWBHDF5IO(NWBdata_concatenate, mode='r')\n",
    "nwbfile = io1.read()\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ee64f3-05a5-4938-a1e5-80adee5024c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "io1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c478b65-101b-4691-8c67-3e4d0a0475f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Timeseries required in the NWB file. \n",
    "\n",
    "ttl_timestamps = np.load(stimTTL)\n",
    "stimulusTTL = TimeSeries(\n",
    "    name=\"TTL\",\n",
    "    data=ttl_timestamps,\n",
    "    unit=\"a.u.\",\n",
    "    timestamps=ttl_timestamps\n",
    ")\n",
    "\n",
    "timestamp = np.load(timestamp)\n",
    "stimulusTimestamp = TimeSeries(\n",
    "    name=\"timestamp\",\n",
    "    data=timestamp,\n",
    "    unit=\"a.u.\",\n",
    "    timestamps=timestamp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef71069-f77a-44d6-b50e-4512ce3039da",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulusTimestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da184b1e-d0c4-4178-81bf-1770f8418636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV PART : Create a csv with the data structure allowing data incorportation in the NWB file. \n",
    "df = pd.read_csv(csv_file_path, sep=';')\n",
    "\n",
    "# Ajouter artificiellement les colonnes 'start_time' et 'stop_time'\n",
    "df['start_time'] = 0 # Fake data \n",
    "df['stop_time'] = 0 # Fake data \n",
    "\n",
    "# Placer les nouvelles colonnes en premire position (optionnel, pour respecter l'ordre)\n",
    "new_columns = ['start_time', 'stop_time'] + [col for col in df.columns if col not in ['start_time', 'stop_time']]\n",
    "df = df[new_columns]\n",
    "\n",
    "# Sauvegarder avec le nouveau header\n",
    "df.to_csv(csv_file_path_copy, sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe5e310-6e30-4fc1-86fc-2f5f3ad87172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIDEO PART : Create video file as timeseries to be integrated in the NWB file.\n",
    "data = loadmat(timing_files)\n",
    "video_timestamps = data['VideoTiming'].squeeze()\n",
    "\n",
    "timestamps_video = TimeSeries(\n",
    "    name=\"timestamps_video\",\n",
    "    data=video_timestamps,\n",
    "    unit=\"s\",\n",
    "    timestamps=video_timestamps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff1f31a-8985-43a4-9379-b1291f425bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroconv.datainterfaces import ExternalVideoInterface, PhySortingInterface, CsvTimeIntervalsInterface #OpenEphysRecordingInterface\n",
    "\n",
    "# Create open ephys conversion interface \n",
    "# interface_openephys = OpenEphysRecordingInterface(folder_path=folder_path_ephys, stream_name=stream_name)\n",
    "\n",
    "# create spike sorting interface \n",
    "interface_phy = PhySortingInterface(folder_path=folder_path_phy, verbose=False)\n",
    "\n",
    "# create video behavior interface\n",
    "interface_behavior = ExternalVideoInterface(file_paths=[video_file_path], verbose=False, video_name=\"BehaviorVideo\")\n",
    "\n",
    "# create csv interface\n",
    "csv_interface = CsvTimeIntervalsInterface(file_path=csv_file_path_copy, verbose=False)\n",
    "\n",
    "with NWBHDF5IO(basil_NWBdata, mode = 'r') as fin, NWBHDF5IO(NWBdata_concatenate, mode = 'w' ) as fout:\n",
    "    datatmp = fin.read() # Read nwb file from BASIL\n",
    "    interface_behavior.add_to_nwbfile(datatmp) # Add the behavior video\n",
    "    datatmp.add_acquisition(timestamps_video) # Add the timestamps cam\n",
    "    interface_phy.add_to_nwbfile(datatmp) # Add the phy information\n",
    "    datatmp.add_acquisition(stimulusTTL) # Add the TTL event\n",
    "    datatmp.add_acquisition(stimulusTimestamp) # Add the timestamp event\n",
    "    csv_interface.add_to_nwbfile(datatmp) # Add csv file\n",
    "    fout.export(fin, nwbfile=datatmp) # Export the new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279847b6-13ca-40d5-bb0e-e24cbae2f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "io =  NWBHDF5IO(NWBdata_concatenate, mode='r')\n",
    "nwbfile = io.read()\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8270eca-e56a-4a46-b740-4b48752fcb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e48a8c",
   "metadata": {},
   "source": [
    "## Other ressources \n",
    "\n",
    "OpenScope Databook (Allen Institute) : https://alleninstitute.github.io/openscope_databook/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nwbworkshop",
   "language": "python",
   "name": "nwbworkshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
