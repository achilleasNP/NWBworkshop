{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54575ef6",
   "metadata": {},
   "source": [
    "# Pre requisites\n",
    "\n",
    "If you were running this on your own machine you would have to install jupyter, neuroconv package and the data.\n",
    "\n",
    "However, if you are using either binder or github codespaces for the workshop these steps have already been done for you so you don't need to install anything.\n",
    "\n",
    "\n",
    "1- Download, install and open GUIDE : https://nwb-guide.readthedocs.io/en/stable/\n",
    "\n",
    "2- Get datasets on your local machine : \n",
    "\n",
    "Download 2 NWB datasets on Pasteur Drive (Total 23MB)\n",
    "\n",
    "https://drive.pasteur.fr/s/ipLGkdP83Mwo975 \n",
    "\n",
    "with the following password: WorkshopNWB@2025\n",
    "\n",
    "\n",
    "# NWB workshop\n",
    "\n",
    "## PART 1 : Reading & Exploring a NWB file \n",
    "\n",
    "### Dataset 1 : Behavioral data (Go/NoGo auditory task)\n",
    "\n",
    "In this section, you will load NWB datasets with the no-code tool \"GUIDE\".\n",
    "\n",
    "Here, we explore a dataset acquired with BASIL app which is used to train mice to operant conditionning auditory discrimination task. One sound (Go) is associated to a reward and another sound (Nogo) is associated to no reward / punishment.\n",
    "\n",
    "1- In the GUIDE app, click Explore, load and find in your local repository the file sub-3502_ses-13.nwb\n",
    "\n",
    "3502 is the subject (mouse) ID.\n",
    "\n",
    "13 is the session (acquisition) ID.\n",
    "\n",
    "2- Expand the Neurodata Tab\n",
    "\n",
    "3- Check the « Lick » and click the blue button on top. Zoom to 20s of experiments. \n",
    "\n",
    "4- Close and now check these 2 items : Lick, Reward. Click the blue button on top. Go a bit after the beginning.  \n",
    "Explanation on how to interpret these data are documented in the \"description\" part.\n",
    "\n",
    "5- Close. Now, in Matlab or Python, with few line of codes, you can open and visualize the data in a notebook as follow : \n",
    "\n",
    "### Look at the data structure and visualize behavior data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "148727d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pynwb import NWBHDF5IO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba2e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"C:/Users/cdussaux/Documents/Python/NWB_conversion/data/sub-3502_ses-13.nwb\"\n",
    "io = NWBHDF5IO(folder_path, mode='r')\n",
    "nwbfile = io.read()\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "be843d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Reward data \n",
    "rewarddata = nwbfile.acquisition['Reward'].data\n",
    "Bin = 1./nwbfile.acquisition['Reward'].rate\n",
    "tps_trial = np.arange(1, len(rewarddata) + 1) * Bin\n",
    "\n",
    "# Load trial type data \n",
    "trialid = nwbfile.acquisition['TrialType'].data\n",
    "\n",
    "# Load Lick data data\n",
    "Lickdata = nwbfile.acquisition['Lick'].data\n",
    "Binlick = 1./nwbfile.acquisition['Lick'].rate\n",
    "tps_data = np.arange(1, len(Lickdata) + 1) * Binlick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e26812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize timeseries\n",
    "plt.plot(tps_data[:], Lickdata[:])\n",
    "plt.plot(tps_trial[:], trialid[:])\n",
    "plt.plot(tps_trial[:], rewarddata[:], \"r-\")\n",
    "\n",
    "plt.ylim([0, 10])\n",
    "plt.xlim([300, 320])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "95bded12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load whichsound.bin - in an ulterior version, it will be included in the NWB. \n",
    "def read_whichsound_bin(fpath):\n",
    "    file_path = fpath + 'WhichSound.bin'\n",
    "    dataid = np.fromfile(file_path, dtype='float64')\n",
    "    return dataid\n",
    "\n",
    "fpath = 'U:/Bathellierlab_gaia/BASIL/BASIL_FAIR/BASILapp/Results/TGTD/NAJ/Ribeye project/Ctrl vs RIb ll inj bilat (deaf)/M3502/20250909/123716_Data/'\n",
    "dataid = read_whichsound_bin(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078dfa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sequence_simple_auto(arr):\n",
    "    \"\"\"\n",
    "    Fonction pour extraire la liste des sons, en incluant les répétitions du fichier whichsound.\n",
    "    \"\"\"\n",
    "    if len(arr) == 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Trouver la longueur minimale des séquences de son\n",
    "    lengths = []\n",
    "    for key, group in groupby(arr):\n",
    "        length = len(list(group))\n",
    "        lengths.append(length)\n",
    "    \n",
    "    # Prendre la longueur minimale (mais au moins 1)\n",
    "    repetition_length = min(lengths) if lengths else 1\n",
    "    \n",
    "    #print(f\"Longueur de répétition détectée: {repetition_length}\")\n",
    "    \n",
    "    result = []\n",
    "    for key, group in groupby(arr):\n",
    "        group_list = list(group)\n",
    "        num_occurrences = len(group_list) // repetition_length\n",
    "        result.extend([key] * num_occurrences)\n",
    "    \n",
    "    return np.array(result)\n",
    "\n",
    "# Extraire la séquence unique des sons de whichsound\n",
    "AllTrials = process_sequence_simple_auto(dataid)\n",
    "print(AllTrials[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5433c572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver le nombre de sons uniques (en ignorant les NaN)\n",
    "valid_trials = AllTrials[~np.isnan(AllTrials)]\n",
    "AllTrials = AllTrials[~np.isnan(AllTrials)]\n",
    "if len(valid_trials) > 0:\n",
    "    SoundNum = int(np.nanmax(np.unique(valid_trials)))\n",
    "else:\n",
    "    SoundNum = 0\n",
    "\n",
    "print(f\"Nombre de sons: {SoundNum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8a402ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On construit le vecteur Correct qui permet de quantifier les performances de l'animal au cours de la session\n",
    "trialid = np.array(trialid)\n",
    "trial_start_id = np.where(trialid == 9)[0]\n",
    "sound_start_id = np.where(trialid == 99)[0]\n",
    "\n",
    "# Create Correct_out with the appropriate length\n",
    "Correct_out = np.zeros(len(sound_start_id), dtype=int)\n",
    "\n",
    "for k in range(len(sound_start_id)):\n",
    "    if k == len(sound_start_id) - 1:\n",
    "        respwin = trialid[trial_start_id[k]+1:]\n",
    "        rewardid = rewarddata[trial_start_id[k]+1:]\n",
    "    else:\n",
    "        # Regular case\n",
    "        respwin = trialid[trial_start_id[k]+1 : trial_start_id[k+1]-1]\n",
    "        rewardid = rewarddata[trial_start_id[k]+1 : trial_start_id[k+1]-1]\n",
    "    \n",
    "    # Supprimer les valeurs 99 qui correspondent au début du son\n",
    "    respwin = respwin[respwin != 99]\n",
    "    \n",
    "    # Trouver les valeurs uniques\n",
    "    outcometrial = np.unique(respwin)\n",
    "    \n",
    "    try:\n",
    "        rewardout = rewardid[rewardid > 0]\n",
    "    except:\n",
    "        rewardout = np.array([0])\n",
    "    \n",
    "    # Supprimer les valeurs 4 qui correspondent à des reward manuel\n",
    "    rewardout = rewardout[rewardout != 4]\n",
    "    \n",
    "    if len(rewardout) == 0:\n",
    "        rewardout = np.array([0])\n",
    "    \n",
    "    # Déterminer si le trial est correct\n",
    "    if (3 in rewardout) or (1 in rewardout):\n",
    "        Correct_out[k] = 1\n",
    "    elif (0 in rewardout) and (2 in outcometrial):\n",
    "        Correct_out[k] = 1\n",
    "    else:\n",
    "        Correct_out[k] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6247eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Correct = np.array(Correct_out)\n",
    "Perf = np.nanmean(Correct)\n",
    "\n",
    "# Calcul des proportions\n",
    "if SoundNum > 0:\n",
    "    PropCorrect = np.full(SoundNum, np.nan)\n",
    "    for sd in range(1, SoundNum + 1):\n",
    "        indices = (AllTrials == sd)\n",
    "        if np.any(indices):\n",
    "            PropCorrect[sd-1] = np.nanmean(Correct[indices])\n",
    "    \n",
    "    # Inverser les proportions pour la première moitié\n",
    "    PropCorrect[0:SoundNum//2] = 1 - PropCorrect[0:SoundNum//2]\n",
    "    \n",
    "    # Graphique\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, SoundNum + 1), PropCorrect, '.-', linewidth=3, markersize=20)\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlabel('Sound Identity')\n",
    "    plt.ylabel('High proba')\n",
    "    plt.axvline(x=SoundNum/2 + 0.5, color='k', linestyle='-')\n",
    "    plt.title(f'Performance: {Perf:.3f}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Aucune donnée à analyser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5b562610",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce114734",
   "metadata": {},
   "source": [
    "### Dataset 2 : Electrophysiological recordings + Sound stimuli\n",
    "\n",
    "Here, we explore a dataset including : \n",
    "\n",
    "- Preprocessing / Spike sorting of Neuropixel recordings acquired with OpenEphys. Preprocessing is done with Kilosort and data curation is performed with Phy tool.  \n",
    "\n",
    "- Behavior/stimuli data acquired with BASIL.\n",
    "\n",
    "- EVENT TTL from BASIL to ephys for offline data synchronization.\n",
    "\n",
    "1- In the GUIDE app, click Explore, load and find in your local repository the file sub-716_ses-27_res.nwb\n",
    "\n",
    "716 is the subject (mouse) ID.\n",
    "\n",
    "27 is the session (acquisition) ID.\n",
    "\n",
    "Dataset courtesy : Pierre Platel, DSAPM.\n",
    "\n",
    "2- Expand the Neurodata Tab\n",
    "\n",
    "3- Expand the « acquisition » Tab. \n",
    "\n",
    "4- Check the « Lick » and click the blue button on top. Zoom to 20s of experiments. \n",
    "\n",
    "5- Close and now check 2 of these items : Lick, TTLTrignpx. Click the blue button on top. Zoom on 50s. \n",
    "\n",
    "6- Close. With few line of codes, you can open and visualize the data in a notebook as follow : \n",
    "\n",
    "### Look at the data structure and visualize behavior data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392885e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"C:/Users/cdussaux/Documents/Python/NWB_conversion/data/sub-716_ses-27_res.nwb\"\n",
    "io = NWBHDF5IO(folder_path, mode='r')\n",
    "nwbfile = io.read()\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f347cef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize timeseries\n",
    "plt.plot(nwbfile.acquisition['Lick'].data[100:20000])\n",
    "plt.plot(nwbfile.acquisition['TTLtrignpx'].data[100:20000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9426f4a3",
   "metadata": {},
   "source": [
    "7- Go back to GUIDE app, expand the «unit» Tab and have a look at the units description. \n",
    "\n",
    "8- Click the \"Raster\" button in the \"units\" checkbox.\n",
    "\n",
    "Select > 100 units with the \"+\" \n",
    "\n",
    "Slide on 10s timewindows.\n",
    "\n",
    "9- Close and go to Python usage. With few line of codes, you can open and visualize the data in a notebook as follow :\n",
    "\n",
    "\n",
    "### Look at the data structure and plot spike raster plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b3782067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "units = nwbfile.units\n",
    "units_spike_times = units[\"spike_times\"]\n",
    "# bin size for counting spikes\n",
    "time_resolution = 0.01\n",
    "\n",
    "# start and end times (relative to the stimulus at 0 seconds) that we want to examine and align spikes to\n",
    "window_start_time = -0.1\n",
    "window_end_time = 2\n",
    "\n",
    "# time bins used\n",
    "n_bins = int((window_end_time - window_start_time) / time_resolution)\n",
    "bin_edges = np.linspace(window_start_time, window_end_time, n_bins, endpoint=True)\n",
    "\n",
    "# useful throughout analysis\n",
    "n_units = len(units_spike_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fc63f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 1\n",
    "\n",
    "# 3D spike matrix to be populated with spike counts\n",
    "spike_matrix = np.zeros((n_units, len(bin_edges), n_trials))\n",
    "\n",
    "# populate 3D spike matrix for each unit for each stimulus trial by counting spikes into bins\n",
    "for unit_idx in range(n_units):\n",
    "    spike_times = units_spike_times[unit_idx]\n",
    "    \n",
    "    # get spike times that fall within the bin's time range relative to the stim time        \n",
    "    first_bin_time = bin_edges[0]\n",
    "    last_bin_time = bin_edges[-1]\n",
    "    first_spike_in_range, last_spike_in_range = np.searchsorted(spike_times, [first_bin_time, last_bin_time])\n",
    "    spike_times_in_range = spike_times[first_spike_in_range:last_spike_in_range]\n",
    "\n",
    "    # convert spike times into relative time bin indices\n",
    "    bin_indices = ((spike_times_in_range - (first_bin_time)) / time_resolution).astype(int)\n",
    "    \n",
    "    # mark that there is a spike at these bin times for this unit on this stim trial\n",
    "    for bin_idx in bin_indices:\n",
    "        spike_matrix[unit_idx, bin_idx, 0] += 1\n",
    "\n",
    "spike_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f633630",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = 0\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "ax.set_title(\"Unit Spikes\")\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax.set_ylabel(\"Unit #\")\n",
    "\n",
    "img = ax.imshow(spike_matrix[:,:,trial], extent=[window_start_time,window_end_time,0,n_units], aspect=0.001, vmin=0, vmax=1)\n",
    "cbar = fig.colorbar(img, shrink=0.5)\n",
    "cbar.set_label(\"# Spikes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "04fa881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12afd128",
   "metadata": {},
   "source": [
    "## PART 2 : Building a multimodal NWB file using neuroconv\n",
    "\n",
    "In this section, you will create the previous NWB dataset 2 using : \n",
    "\n",
    "1- NWB BASIL file\n",
    "\n",
    "2- RAW ephys file (just for the example but too heavy)\n",
    "\n",
    "3- RAW Kilosort + Phy file\n",
    "\n",
    "4- EVENT TTL from BASIL to ephys for synchronisation\n",
    "\n",
    "5- Behavior video file as external link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2a38bd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pathlib import Path\n",
    "from pynwb import NWBHDF5IO\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pynwb import TimeSeries\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1800459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide saving and data paths\n",
    "saving_data_dir = Path(\"C:/Users/cdussaux/Documents/Python/NWB_conversion/data\")\n",
    "\n",
    "# Select BASIL NWB file \n",
    "basil_NWB_dir = Path(\"C:/Users/cdussaux/Documents/Python/NWB_workshop/NWBworkshop/data\")\n",
    "basil_NWB_filename = \"sub-716_ses-27.nwb\" \n",
    "basil_NWBdata = basil_NWB_dir / basil_NWB_filename\n",
    "\n",
    "# Select open ephys RAW data file (folder and stream) \n",
    "#folder_path_ephys = \"U:/Bathellierlab_gaia/Data/Electrophysiology/Pierre/RAW_DATA/716_20250915_DelGNG_AC&M2_3&3/Record Node 112\"\n",
    "#stream_name = 'Record Node 112#Neuropix-PXI-116.ProbeB-AP'\n",
    "\n",
    "# Select RAW preprocessed data (spike sorting)\n",
    "folder_path_phy = \"C:/Users/cdussaux/Documents/Python/NWB_workshop/NWBworkshop/data/716_20250915_DelGNG_M2_3\"\n",
    "\n",
    "# Select event TTL sent by BASIL to open ephys \n",
    "stimTTLtmp = \"C:/Users/cdussaux/Documents/Python/NWB_workshop/NWBworkshop/data/stimTTL/timestamps.npy\" #\"/experiment1/recording1/events/Neuropix-PXI-116.ProbeA-AP/TTL/timestamps.npy\"\n",
    "stimTTL = stimTTLtmp #folder_path_ephys + stimTTLtmp\n",
    "\n",
    "# Select NPX timestamp\n",
    "timestamptmp = \"C:/Users/cdussaux/Documents/Python/NWB_workshop/NWBworkshop/data/timestamp/timestamps.npy\" #\"/experiment1/recording1/continuous/Neuropix-PXI-116.ProbeA-AP/timestamps.npy\"\n",
    "timestamp = timestamptmp # folder_path_ephys + timestamptmp\n",
    "\n",
    "# Select Video file\n",
    "raw_video_path = \"C:/Users/cdussaux/Documents/Python/NWB_workshop/NWBworkshop/data/video/\"  #\"U:/Bathellierlab_gaia/BASIL/BASIL_FAIR/BASILapp/Results/Pierre/M716/20250915/143731_Data/\"\n",
    "video_file_path = raw_video_path + \"MouseVideo1.avi\"\n",
    "timing_files = raw_video_path + \"VideoTiming1.mat\"\n",
    "\n",
    "# Add csv log file to NWB\n",
    "csv_path = \"C:/Users/cdussaux/Documents/Python/NWB_workshop/NWBworkshop/data/\"\n",
    "csv_file_path = csv_path + \"Trial_log.csv\"\n",
    "csv_file_path_copy = csv_path + \"Trial_log_copy.csv\"\n",
    "\n",
    "# name of the new file is duplicated in the saving_data_dir.\n",
    "saving_data_filename, ext = os.path.splitext(basil_NWB_filename)\n",
    "saving_data_filename = saving_data_filename + \"_res\" + ext\n",
    "NWBdata_concatenate = saving_data_dir / saving_data_filename\n",
    "NWBdata_concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce69ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of BASIL nwb file\n",
    "shutil.copyfile(basil_NWBdata, NWBdata_concatenate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2625f63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "io1 =  NWBHDF5IO(NWBdata_concatenate, mode='r')\n",
    "nwbfile = io1.read()\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "aff2480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "io1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "df5e0ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Timeseries required in the NWB file. \n",
    "\n",
    "ttl_timestamps = np.load(stimTTL)\n",
    "stimulusTTL = TimeSeries(\n",
    "    name=\"TTL\",\n",
    "    data=ttl_timestamps,\n",
    "    unit=\"a.u.\",\n",
    "    timestamps=ttl_timestamps\n",
    ")\n",
    "\n",
    "timestamp = np.load(timestamp)\n",
    "stimulusTimestamp = TimeSeries(\n",
    "    name=\"timestamp\",\n",
    "    data=timestamp,\n",
    "    unit=\"a.u.\",\n",
    "    timestamps=timestamp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c78ce54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV PART : Create a csv with the data structure allowing data incorportation in the NWB file. \n",
    "df = pd.read_csv(csv_file_path, sep=';')\n",
    "\n",
    "# Ajouter artificiellement les colonnes 'start_time' et 'stop_time'\n",
    "df['start_time'] = 0 # Fake data \n",
    "df['stop_time'] = 0 # Fake data \n",
    "\n",
    "# Placer les nouvelles colonnes en première position (optionnel, pour respecter l'ordre)\n",
    "new_columns = ['start_time', 'stop_time'] + [col for col in df.columns if col not in ['start_time', 'stop_time']]\n",
    "df = df[new_columns]\n",
    "\n",
    "# Sauvegarder avec le nouveau header\n",
    "df.to_csv(csv_file_path_copy, sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "030d9d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIDEO PART : Create video file as timeseries to be integrated in the NWB file.\n",
    "data = loadmat(timing_files)\n",
    "video_timestamps = data['VideoTiming'].squeeze()\n",
    "\n",
    "timestamps_video = TimeSeries(\n",
    "    name=\"timestamps_video\",\n",
    "    data=video_timestamps,\n",
    "    unit=\"s\",\n",
    "    timestamps=video_timestamps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6cb3147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroconv.datainterfaces import ExternalVideoInterface, PhySortingInterface, CsvTimeIntervalsInterface #OpenEphysRecordingInterface\n",
    "\n",
    "# Create open ephys conversion interface \n",
    "# interface_openephys = OpenEphysRecordingInterface(folder_path=folder_path_ephys, stream_name=stream_name)\n",
    "\n",
    "# create spike sorting interface \n",
    "interface_phy = PhySortingInterface(folder_path=folder_path_phy, verbose=False)\n",
    "\n",
    "# create video behavior interface\n",
    "interface_behavior = ExternalVideoInterface(file_paths=[video_file_path], verbose=False, video_name=\"BehaviorVideo\")\n",
    "\n",
    "# create csv interface\n",
    "csv_interface = CsvTimeIntervalsInterface(file_path=csv_file_path_copy, verbose=False)\n",
    "\n",
    "with NWBHDF5IO(basil_NWBdata, mode = 'r') as fin, NWBHDF5IO(NWBdata_concatenate, mode = 'w' ) as fout:\n",
    "    datatmp = fin.read() # Read nwb file from BASIL\n",
    "    interface_behavior.add_to_nwbfile(datatmp) # Add the behavior video\n",
    "    datatmp.add_acquisition(timestamps_video) # Add the timestamps cam\n",
    "    interface_phy.add_to_nwbfile(datatmp) # Add the phy information\n",
    "    datatmp.add_acquisition(stimulusTTL) # Add the TTL event\n",
    "    datatmp.add_acquisition(stimulusTimestamp) # Add the timestamp event\n",
    "    csv_interface.add_to_nwbfile(datatmp) # Add csv file\n",
    "    fout.export(fin, nwbfile=datatmp) # Export the new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6309d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "io =  NWBHDF5IO(NWBdata_concatenate, mode='r')\n",
    "nwbfile = io.read()\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a5354a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271d6a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroconv-G-QZBxTl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
